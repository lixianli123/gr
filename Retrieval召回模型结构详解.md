# ğŸ” Retrievalå¬å›æ¨¡å‹ç»“æ„è¯¦è§£

## ğŸ“‹ ç›®å½•
1. [æ¨¡å‹æ•´ä½“ç»“æ„](#æ¨¡å‹æ•´ä½“ç»“æ„)
2. [ä¸Rankingæ¨¡å‹çš„åŒºåˆ«](#ä¸rankingæ¨¡å‹çš„åŒºåˆ«)
3. [ä»£ç å±‚çº§ç»“æ„](#ä»£ç å±‚çº§ç»“æ„)
4. [æ ¸å¿ƒç±»å®šä¹‰](#æ ¸å¿ƒç±»å®šä¹‰)
5. [å‰å‘ä¼ æ’­æµç¨‹](#å‰å‘ä¼ æ’­æµç¨‹)
6. [é…ç½®æ–¹å¼](#é…ç½®æ–¹å¼)
7. [åŒå¡”ç»“æ„è¯¦è§£](#åŒå¡”ç»“æ„è¯¦è§£)

---

## æ¨¡å‹æ•´ä½“ç»“æ„

### æ¶æ„å›¾

```
è¾“å…¥æ•°æ® (RetrievalBatch)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              RetrievalGR æ¨¡å‹ (åŒå¡”ç»“æ„)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  1. Embeddingå±‚ (ShardedEmbedding)             â”‚   â”‚
â”‚  â”‚     - Contextual Embedding (ç”¨æˆ·ç‰¹å¾)          â”‚   â”‚
â”‚  â”‚     - Item Embedding (ç‰©å“ç‰¹å¾)                â”‚   â”‚
â”‚  â”‚     - Action Embedding (åŠ¨ä½œç‰¹å¾)              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                        â†“                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  2. HSTU Block (HSTUBlock)                     â”‚   â”‚
â”‚  â”‚     - Preprocessing (åºåˆ—æ‹¼æ¥ã€ä½ç½®ç¼–ç )       â”‚   â”‚
â”‚  â”‚     - Multi-layer HSTU Attention               â”‚   â”‚
â”‚  â”‚       * FusedHSTULayer (CUTLASSåŠ é€Ÿ)           â”‚   â”‚
â”‚  â”‚       * LayerNorm + Linear + SiLU              â”‚   â”‚
â”‚  â”‚       * HSTU Attention                          â”‚   â”‚
â”‚  â”‚     - Postprocessing (æå–item embeddings)     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                        â†“                                 â”‚
â”‚              Splitä¸ºä¸¤éƒ¨åˆ† (åŒå¡”)                         â”‚
â”‚                        â†“                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Query Tower        â”‚  Item Tower             â”‚     â”‚
â”‚  â”‚  (å†å²å‰n-1ä¸ª)      â”‚  (ç›‘ç£ä¿¡å·ï¼šæœ€å1ä¸ª)    â”‚     â”‚
â”‚  â”‚  [BS, hidden_size]  â”‚  [BS, hidden_size]      â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                        â†“                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  3. L2å½’ä¸€åŒ– (L2NormEmbeddingPostprocessor)    â”‚   â”‚
â”‚  â”‚     normalize(query_emb)                        â”‚   â”‚
â”‚  â”‚     normalize(item_emb)                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                        â†“                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  4. ç›¸ä¼¼åº¦è®¡ç®— (DotProductSimilarity)          â”‚   â”‚
â”‚  â”‚     scores = query_emb @ item_emb.T             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                        â†“                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  5. Lossè®¡ç®— (SampledSoftmaxLoss)              â”‚   â”‚
â”‚  â”‚     - InBatchNegativesSampler (è´Ÿæ ·æœ¬é‡‡æ ·)     â”‚   â”‚
â”‚  â”‚     - Softmax with temperature                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
è¾“å‡º: Similarity Scores + Loss
```

---

## ä¸Rankingæ¨¡å‹çš„åŒºåˆ«

### å¯¹æ¯”è¡¨

| ç‰¹æ€§ | Rankingæ¨¡å‹ | Retrievalæ¨¡å‹ |
|------|------------|--------------|
| **ä»»åŠ¡ç›®æ ‡** | é¢„æµ‹è¯„åˆ†/ç‚¹å‡»ç‡ | ä»å€™é€‰æ± ä¸­å¬å›topKç‰©å“ |
| **è¾“å‡ºå½¢å¼** | Logits (è¯„åˆ†ç±»åˆ«æ¦‚ç‡) | Embedding (å‘é‡è¡¨ç¤º) |
| **é¢„æµ‹å¤´** | MLP (å¤šå±‚å…¨è¿æ¥) | æ—  (ç›´æ¥ä½¿ç”¨HSTUè¾“å‡º) |
| **Losså‡½æ•°** | BCE Loss / Cross Entropy | Sampled Softmax Loss |
| **ç›¸ä¼¼åº¦è®¡ç®—** | æ—  | ç‚¹ç§¯ç›¸ä¼¼åº¦ (Dot Product) |
| **è´Ÿæ ·æœ¬** | æ—  | In-Batch Negatives |
| **å½’ä¸€åŒ–** | æ—  | L2å½’ä¸€åŒ– (å¿…éœ€) |
| **ç»“æ„** | å•å¡” | åŒå¡” (Query + Item) |
| **æ¨ç†æ–¹å¼** | åœ¨çº¿è®¡ç®— | ç¦»çº¿æ„å»ºç´¢å¼• + ANNæ£€ç´¢ |
| **å…¸å‹è¯„ä¼°æŒ‡æ ‡** | AUC, LogLoss | NDCG@K, HR@K, Recall@K |

### å…³é”®å·®å¼‚ç‚¹

#### 1. **æ— MLPé¢„æµ‹å¤´**
```python
# Rankingæ¨¡å‹æœ‰:
self._mlp = MLP(...)
logits = self._mlp(hidden_states)

# Retrievalæ¨¡å‹æ²¡æœ‰:
# ç›´æ¥ä½¿ç”¨HSTU Blockè¾“å‡ºçš„embedding
pred_item_embeddings = jagged_data.values
```

#### 2. **åŒå¡”ç»“æ„**
```python
# Retrievalæ¨¡å‹å°†åºåˆ—åˆ†ä¸ºä¸¤éƒ¨åˆ†:
# Query Tower: å†å²åºåˆ—å‰n-1ä¸ªç‰©å“ â†’ é¢„æµ‹
# Item Tower: æœ€å1ä¸ªç‰©å“ â†’ ç›‘ç£ä¿¡å· (æ­£æ ·æœ¬)

# è®­ç»ƒæ—¶è®¡ç®—ç›¸ä¼¼åº¦: query_emb @ item_emb
# æ¨ç†æ—¶: query_emb @ all_item_embs (ä»ç‰©å“åº“æ£€ç´¢)
```

#### 3. **Sampled Softmax Loss**
```python
# Ranking: BCE Loss
losses = BCE(logits, labels)

# Retrieval: Sampled Softmax Loss
losses = SampledSoftmax(
    query_emb,           # [BS, D]
    positive_item_emb,   # [BS, D] æ­£æ ·æœ¬
    negative_item_embs,  # [BS, N, D] è´Ÿæ ·æœ¬
)
# Loss = -log(exp(sim(q, pos)) / (exp(sim(q, pos)) + Î£exp(sim(q, neg))))
```

---

## ä»£ç å±‚çº§ç»“æ„

### æ–‡ä»¶ç»„ç»‡

```
examples/hstu/
â”œâ”€â”€ pretrain_gr_retrieval.py        # è®­ç»ƒå…¥å£
â”œâ”€â”€ movielen_retrieval.gin          # æ¨¡å‹é…ç½®æ–‡ä»¶
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ __init__.py                 # get_retrieval_model()
â”‚   â”œâ”€â”€ base_model.py               # BaseModelåŸºç±»
â”‚   â””â”€â”€ retrieval_gr.py             # â­ RetrievalGRæ ¸å¿ƒç±»
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ embedding.py                # ShardedEmbedding
â”‚   â”œâ”€â”€ hstu_block.py               # HSTUBlock
â”‚   â”œâ”€â”€ sampled_softmax_loss.py     # SampledSoftmaxLoss
â”‚   â”œâ”€â”€ negatives_sampler.py        # InBatchNegativesSampler
â”‚   â”œâ”€â”€ output_postprocessors.py    # L2NormEmbeddingPostprocessor
â”‚   â””â”€â”€ similarity/
â”‚       â””â”€â”€ dot_product.py          # DotProductSimilarity
â””â”€â”€ configs/
    â”œâ”€â”€ task_config.py              # RetrievalConfig
    â””â”€â”€ hstu_config.py              # HSTUConfig
```

---

## æ ¸å¿ƒç±»å®šä¹‰

### 1. RetrievalGR (model/retrieval_gr.py)

è¿™æ˜¯**å¬å›æ¨¡å‹çš„æ ¸å¿ƒç±»**ï¼Œå®šä¹‰äº†"Embedding â†’ HSTU â†’ Similarity â†’ Loss"ç»“æ„ã€‚

```python
class RetrievalGR(BaseModel):
    """
    Retrievalç”Ÿæˆæ¨èæ¨¡å‹ (åŒå¡”ç»“æ„)
    
    ç»“æ„:
        self._embedding_collection  # Embeddingå±‚
        self._hstu_block            # HSTU Block
        self._loss_module           # Sampled Softmax Loss
            â”œâ”€ negatives_sampler    # In-Batchè´Ÿæ ·æœ¬é‡‡æ ·å™¨
            â”‚   â””â”€ norm_func        # L2å½’ä¸€åŒ–
            â””â”€ interaction_module   # ç‚¹ç§¯ç›¸ä¼¼åº¦
    """
    
    def __init__(
        self,
        hstu_config: HSTUConfig,        # HSTUé…ç½®
        task_config: RetrievalConfig,   # Retrievalä»»åŠ¡é…ç½®
    ):
        super().__init__()
        
        # æ£€æŸ¥: Retrievalä¸æ”¯æŒå¼ é‡å¹¶è¡Œ
        assert self._tp_size == 1, \
            "RetrievalGR does not support tensor model parallel"
        
        self._embedding_dim = hstu_config.hidden_size  # embeddingç»´åº¦
        
        # ç¬¬1å±‚: Embeddingå±‚
        self._embedding_collection = ShardedEmbedding(
            task_config.embedding_configs
        )
        
        # ç¬¬2å±‚: HSTU Block (ä¸Rankingç›¸åŒ)
        self._hstu_block = HSTUBlock(hstu_config)
        
        # ç¬¬3å±‚: Sampled Softmax Loss (æ ¸å¿ƒå·®å¼‚)
        self._loss_module = SampledSoftmaxLoss(
            num_to_sample=task_config.num_negatives,      # è´Ÿæ ·æœ¬æ•° 128
            softmax_temperature=task_config.temperature,   # æ¸©åº¦ç³»æ•° 0.05
            
            # è´Ÿæ ·æœ¬é‡‡æ ·å™¨ (ä»Batchå†…é‡‡æ ·)
            negatives_sampler=InBatchNegativesSampler(
                # L2å½’ä¸€åŒ–å‡½æ•°
                norm_func=L2NormEmbeddingPostprocessor(
                    embedding_dim=self._embedding_dim,
                    eps=task_config.l2_norm_eps,  # 1e-6
                ),
                dedup_embeddings=True,  # å»é‡
            ),
            
            # ç›¸ä¼¼åº¦è®¡ç®—æ¨¡å— (ç‚¹ç§¯)
            interaction_module=DotProductSimilarity(
                dtype=torch.bfloat16 if hstu_config.bf16 else torch.float16
            ),
        )
```

---

### 2. RetrievalGRçš„å‰å‘ä¼ æ’­ (forward)

```python
def forward(self, batch: RetrievalBatch):
    """
    å®Œæ•´çš„å‰å‘ä¼ æ’­æµç¨‹
    
    Args:
        batch (RetrievalBatch): åŒ…å«featuresçš„æ‰¹æ¬¡æ•°æ®
        
    Returns:
        losses: æŸå¤±å€¼
        (losses, logits, supervision_item_ids, seqlen): ç”¨äºæ—¥å¿—å’Œè¯„ä¼°
    """
    # 1. è·å–query embeddingå’Œitem embedding
    (
        jagged_item_logit,         # query embedding [BS, D]
        seqlen_after_preprocessor, # åºåˆ—é•¿åº¦ä¿¡æ¯
        supervision_item_ids,      # ç›‘ç£ç‰©å“ID [BS]
        supervision_emb,           # ç›‘ç£ç‰©å“embedding [BS, D]
    ) = self.get_logit_and_labels(batch)
    
    # 2. è®¡ç®—Sampled Softmax Loss
    losses = self._loss_module(
        jagged_item_logit.float(),    # query embedding
        supervision_item_ids,          # æ­£æ ·æœ¬ç‰©å“ID
        supervision_emb.float(),       # æ­£æ ·æœ¬ç‰©å“embedding
    )
    # å†…éƒ¨æµç¨‹:
    #   - é‡‡æ ·è´Ÿæ ·æœ¬ (In-Batch Negatives)
    #   - L2å½’ä¸€åŒ– query_emb å’Œ item_embs
    #   - è®¡ç®—ç›¸ä¼¼åº¦ scores = query_emb @ item_embs.T
    #   - Softmax with temperature
    #   - è®¡ç®—äº¤å‰ç†µ
    
    # 3. è¿”å›losså’Œç”¨äºè¯„ä¼°çš„ä¿¡æ¯
    return losses, (
        losses.detach(),
        jagged_item_logit.detach(),
        supervision_item_ids.detach(),
        seqlen_after_preprocessor,
    )
```

---

### 3. get_logit_and_labels (æ ¸å¿ƒé€»è¾‘)

è¿™æ˜¯**åŒå¡”ç»“æ„æœ€æ¸…æ™°çš„åœ°æ–¹**ï¼š

```python
def get_logit_and_labels(self, batch: RetrievalBatch):
    """
    å®Œæ•´çš„ Embedding â†’ HSTU â†’ SplitåŒå¡” æµç¨‹
    """
    
    # ========================================
    # ç¬¬1æ­¥: Embeddingå±‚
    # ========================================
    embeddings = self._embedding_collection(batch.features)
    # embeddings = {
    #     "contextual": JaggedTensor,
    #     "item": JaggedTensor,
    #     "action": JaggedTensor,
    # }
    
    # ========================================
    # ç¬¬2æ­¥: HSTU Block
    # ========================================
    jagged_data, seqlen_after_preprocessor = self._hstu_block(
        embeddings=embeddings,
        batch=batch,
    )
    # è¾“å‡º: æ‰€æœ‰item tokençš„hidden states
    pred_item_embeddings = jagged_data.values  # [total_items, D]
    pred_item_seqlen = jagged_data.seqlen      # æ¯ä¸ªæ ·æœ¬çš„åºåˆ—é•¿åº¦
    
    # ========================================
    # ç¬¬3æ­¥: è·å–ç›‘ç£ä¿¡å· (æ­£æ ·æœ¬item embedding)
    # ========================================
    # ä»åŸå§‹embeddingè¡¨ä¸­æŸ¥è¯¢ç›‘ç£ç‰©å“çš„embedding
    supervision_item_embeddings = embeddings[
        batch.item_feature_name
    ].values()
    supervision_item_ids = batch.features[
        batch.item_feature_name
    ].values()
    
    # ========================================
    # ç¬¬4æ­¥: SplitåŒå¡”
    # ========================================
    # Query Tower: å†å²åºåˆ—å‰n-1ä¸ªç‰©å“çš„HSTUè¾“å‡º
    # Item Tower: æœ€å1ä¸ªç‰©å“çš„åŸå§‹embedding (ç›‘ç£ä¿¡å·)
    
    # è®¡ç®—åç§»é‡: æ¯ä¸ªæ ·æœ¬ä¿ç•™å‰n-1ä¸ª
    shift_pred_item_seqlen_offsets = length_to_complete_offsets(
        torch.clamp(pred_item_seqlen - 1, min=0)
    )
    
    # Split: å‰n-1ä¸ª vs æœ€å1ä¸ª
    first_n_pred_item_embeddings, _ = triton_split_2D_jagged(
        pred_item_embeddings,
        pred_item_max_seqlen,
        offsets_a=shift_pred_item_seqlen_offsets,      # å‰n-1ä¸ª
        offsets_b=pred_item_seqlen_offsets - shift_..., # æœ€å1ä¸ª
    )
    
    # åŒæ ·splitç›‘ç£ä¿¡å·
    _, last_n_supervision_item_embeddings = triton_split_2D_jagged(
        supervision_item_embeddings, ...
    )
    _, last_n_supervision_item_ids = triton_split_2D_jagged(
        supervision_item_ids.view(-1, 1), ...
    )
    
    # ========================================
    # è¿”å›åŒå¡”embedding
    # ========================================
    return (
        first_n_pred_item_embeddings.view(-1, self._embedding_dim),  # Queryå¡”
        seqlen_after_preprocessor,
        last_n_supervision_item_ids.view(-1),                        # æ­£æ ·æœ¬ID
        last_n_supervision_item_embeddings.view(-1, self._embedding_dim), # Itemå¡”
    )
```

---

## å‰å‘ä¼ æ’­æµç¨‹

### è¯¦ç»†æ•°æ®æµ

```python
# ============================================
# è¾“å…¥: RetrievalBatch
# ============================================
batch.features = {
    "contextual": KeyedJaggedTensor,  # ç”¨æˆ·ç‰¹å¾
    "item": KeyedJaggedTensor,        # ç‰©å“åºåˆ— [itemâ‚, itemâ‚‚, ..., itemâ‚™]
    "action": KeyedJaggedTensor,      # åŠ¨ä½œåºåˆ—
}
# æ³¨æ„: æ²¡æœ‰labels! (Retrievalä»»åŠ¡çš„æ ‡ç­¾æ˜¯éšå¼çš„)

# ============================================
# ç¬¬1å±‚: ShardedEmbedding
# ============================================
embeddings = self._embedding_collection(batch.features)
# åŒRankingæ¨¡å‹

# ============================================
# ç¬¬2å±‚: HSTUBlock
# ============================================
jagged_data, seqlen = self._hstu_block(
    embeddings=embeddings,
    batch=batch,
)
# è¾“å‡º: pred_item_embeddings [total_items, hidden_size=256]
# åŒ…å«æ‰€æœ‰item tokençš„embedding

# ============================================
# ç¬¬3å±‚: SplitåŒå¡”
# ============================================
# è®­ç»ƒæ ·æœ¬: [itemâ‚, itemâ‚‚, itemâ‚ƒ, ..., itemâ‚™]
# 
# Query Tower: ä½¿ç”¨å‰n-1ä¸ªç‰©å“é¢„æµ‹ä¸‹ä¸€ä¸ª
#   HSTUè¾“å‡º: [embâ‚, embâ‚‚, ..., embâ‚™â‚‹â‚]
#   query_emb = embâ‚™â‚‹â‚ (å–æœ€åä¸€ä¸ªä½œä¸ºquery)
#
# Item Tower: æœ€å1ä¸ªç‰©å“ä½œä¸ºç›‘ç£ä¿¡å·
#   æ­£æ ·æœ¬: itemâ‚™ çš„embedding
#   è´Ÿæ ·æœ¬: ä»Batchå†…å…¶ä»–æ ·æœ¬çš„itemâ‚™é‡‡æ ·

# Splitæ“ä½œ:
first_n_pred_item_embeddings: [BS*(n-1), D]  # å‰n-1ä¸ª
last_n_supervision_item_embeddings: [BS, D]   # æœ€å1ä¸ª

# å®é™…ä¸Šå–æœ€åä¸€ä¸ªä½œä¸ºquery:
query_emb = first_n_pred_item_embeddings[-1::n-1]  # [BS, D]

# ============================================
# ç¬¬4å±‚: L2å½’ä¸€åŒ–
# ============================================
# åœ¨SampledSoftmaxLosså†…éƒ¨è‡ªåŠ¨æ‰§è¡Œ
query_emb = normalize(query_emb, dim=-1)              # [BS, D]
positive_item_emb = normalize(positive_item_emb, dim=-1)  # [BS, D]

# ============================================
# ç¬¬5å±‚: è´Ÿæ ·æœ¬é‡‡æ ·
# ============================================
# InBatchNegativesSamplerä»Batchå†…é‡‡æ ·
# negative_item_embs = [item_emb_1, item_emb_2, ..., item_emb_128]
# shape: [BS, num_negatives=128, D]

# ============================================
# ç¬¬6å±‚: ç›¸ä¼¼åº¦è®¡ç®—
# ============================================
# æ­£æ ·æœ¬ç›¸ä¼¼åº¦
pos_scores = query_emb * positive_item_emb  # [BS, D]
pos_scores = pos_scores.sum(dim=-1)        # [BS]

# è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦
neg_scores = query_emb @ negative_item_embs.T  # [BS, num_negatives]

# åˆå¹¶
all_scores = torch.cat([pos_scores.unsqueeze(1), neg_scores], dim=1)
# shape: [BS, 1+num_negatives]

# ============================================
# ç¬¬7å±‚: Softmax with temperature
# ============================================
all_scores = all_scores / temperature  # temperature=0.05
probs = softmax(all_scores, dim=-1)    # [BS, 1+num_negatives]

# ============================================
# ç¬¬8å±‚: Lossè®¡ç®—
# ============================================
# æ­£æ ·æœ¬çš„æ ‡ç­¾æ˜¯0 (ç¬¬ä¸€ä¸ªä½ç½®)
labels = torch.zeros(BS, dtype=torch.long)

# äº¤å‰ç†µ
loss = CrossEntropy(probs, labels)
# = -log(probs[:, 0])  # æœ€å¤§åŒ–æ­£æ ·æœ¬çš„æ¦‚ç‡
```

---

## é…ç½®æ–¹å¼

### 1. Giné…ç½®æ–‡ä»¶ (movielen_retrieval.gin)

```python
# ========================================
# ç½‘ç»œç»“æ„é…ç½®
# ========================================
NetworkArgs.dtype_str = "bfloat16"
NetworkArgs.num_layers = 4               # HSTUå±‚æ•° (æ¯”Rankingå¤š)
NetworkArgs.num_attention_heads = 4
NetworkArgs.hidden_size = 256            # embeddingç»´åº¦ (å¿…é¡»ä¸€è‡´)
NetworkArgs.kv_channels = 64
NetworkArgs.is_causal = True             # ä½¿ç”¨å› æœmask

# ========================================
# Retrievalä»»åŠ¡é…ç½®
# ========================================
RetrievalArgs.num_negatives = 128        # è´Ÿæ ·æœ¬æ•° â† å…³é”®å‚æ•°
RetrievalArgs.temperature = 0.05         # Softmaxæ¸©åº¦ â† æ§åˆ¶åˆ†å¸ƒå¹³æ»‘åº¦
RetrievalArgs.l2_norm_eps = 1e-6         # L2å½’ä¸€åŒ–epsilon
RetrievalArgs.eval_metrics = ("NDCG@10", "NDCG@20", "HR@10")  # è¯„ä¼°æŒ‡æ ‡

# æ³¨æ„: æ²¡æœ‰prediction_head_arch! (Retrievalä¸éœ€è¦MLP)
```

### 2. æ¨¡å‹å®ä¾‹åŒ– (pretrain_gr_retrieval.py)

```python
# ç¬¬1æ­¥: è§£æé…ç½®æ–‡ä»¶
gin.parse_config_file(args.gin_config_file)

# ç¬¬2æ­¥: åˆ›å»ºé…ç½®å¯¹è±¡
retrieval_args = RetrievalArgs()
network_args = NetworkArgs()

# ç¬¬3æ­¥: åˆ›å»ºHSTUé…ç½®
hstu_config = create_hstu_config(network_args, tp_args)

# ç¬¬4æ­¥: åˆ›å»ºRetrievalé…ç½®
retrieval_config = RetrievalConfig(
    embedding_configs=create_embedding_config(...),
    temperature=0.05,               # Softmaxæ¸©åº¦
    l2_norm_eps=1e-6,               # L2å½’ä¸€åŒ–
    num_negatives=128,              # è´Ÿæ ·æœ¬æ•°
    eval_metrics=("NDCG@10", "HR@10"),
)

# ç¬¬5æ­¥: å®ä¾‹åŒ–æ¨¡å‹
model = get_retrieval_model(
    hstu_config=hstu_config,
    task_config=retrieval_config,
)
# â†’ è¿”å› RetrievalGR å®ä¾‹
# â†’ å†…éƒ¨è‡ªåŠ¨åˆ›å»º: Embedding + HSTU Block + Sampled Softmax Loss
```

---

## åŒå¡”ç»“æ„è¯¦è§£

### è®­ç»ƒé˜¶æ®µ

```python
# è¾“å…¥åºåˆ—: [itemâ‚, itemâ‚‚, itemâ‚ƒ, itemâ‚„, itemâ‚…]
#
# ç»è¿‡HSTU Block:
#   output: [hâ‚, hâ‚‚, hâ‚ƒ, hâ‚„, hâ‚…]  (hidden states)
#
# åŒå¡”split:
#   Query Tower:  [hâ‚, hâ‚‚, hâ‚ƒ, hâ‚„]  â†’ å–hâ‚„ä½œä¸ºquery_emb
#   Item Tower:   åŸå§‹itemâ‚…çš„embedding â†’ positive_item_emb
#
# è®­ç»ƒç›®æ ‡: è®©query_embæ¥è¿‘positive_item_emb
```

### ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ

#### 1. **Query Towerä½¿ç”¨HSTUè¾“å‡º**
- **åŸå› **: HSTUå»ºæ¨¡äº†åºåˆ—ä¾èµ–ï¼ŒåŒ…å«äº†å†å²è¡Œä¸ºä¿¡æ¯
- **ä¼˜åŠ¿**: query_emb = f(itemâ‚, itemâ‚‚, ..., itemâ‚™â‚‹â‚) åŒ…å«ä¸°å¯Œä¸Šä¸‹æ–‡

#### 2. **Item Towerä½¿ç”¨åŸå§‹Embedding**
- **åŸå› **: æ¨ç†æ—¶éœ€è¦ä¸ºæ‰€æœ‰ç‰©å“æ„å»ºembeddingç´¢å¼•
- **æŒ‘æˆ˜**: å¦‚æœItem Towerä¹Ÿç”¨HSTUï¼Œæ¯ä¸ªç‰©å“çš„è¡¨ç¤ºä¼šä¾èµ–ä¸Šä¸‹æ–‡ï¼Œæ— æ³•é¢„å…ˆè®¡ç®—
- **è§£å†³**: ä½¿ç”¨åŸå§‹embeddingï¼Œå¯ä»¥ç¦»çº¿æ„å»ºå›ºå®šçš„ç‰©å“ç´¢å¼•

### æ¨ç†é˜¶æ®µ

```python
# ç¬¬1æ­¥: ç¦»çº¿æ„å»ºç‰©å“ç´¢å¼•
all_item_embs = embedding_table["item"]  # [num_items, D]
all_item_embs = normalize(all_item_embs, dim=-1)  # L2å½’ä¸€åŒ–
# å­˜å…¥å‘é‡æ•°æ®åº“ (å¦‚Faiss, HNSW)

# ç¬¬2æ­¥: åœ¨çº¿è®¡ç®—query embedding
user_history = [itemâ‚, itemâ‚‚, itemâ‚ƒ, itemâ‚„]
query_emb = hstu_block(user_history)  # [1, D]
query_emb = normalize(query_emb, dim=-1)

# ç¬¬3æ­¥: ANNæ£€ç´¢topK
scores = query_emb @ all_item_embs.T  # [1, num_items]
topk_indices = scores.topk(k=100).indices  # å¬å›top-100

# ç¬¬4æ­¥: è¿”å›å¬å›ç»“æœ
recommended_items = [item_ids[i] for i in topk_indices]
```

---

## æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 1. SampledSoftmaxLoss

```python
class SampledSoftmaxLoss(nn.Module):
    """
    å¸¦è´Ÿæ ·æœ¬é‡‡æ ·çš„Softmax Loss
    
    å…¬å¼:
        Loss = -log(exp(sim(q, pos)) / Z)
        Z = exp(sim(q, pos)) + Î£áµ¢ exp(sim(q, negáµ¢))
    
    ä½œç”¨: è®©query_embæ¥è¿‘positive_item_embï¼Œè¿œç¦»negative_item_embs
    """
    
    def forward(
        self,
        query_emb,           # [BS, D]
        positive_item_ids,   # [BS]
        positive_item_emb,   # [BS, D]
    ):
        # 1. é‡‡æ ·è´Ÿæ ·æœ¬
        negative_item_embs = self.negatives_sampler(
            positive_item_ids,
            positive_item_emb,
        )  # [BS, num_negatives, D]
        
        # 2. L2å½’ä¸€åŒ–
        query_emb = self.norm_func(query_emb)
        positive_item_emb = self.norm_func(positive_item_emb)
        negative_item_embs = self.norm_func(negative_item_embs)
        
        # 3. è®¡ç®—ç›¸ä¼¼åº¦
        pos_scores = (query_emb * positive_item_emb).sum(-1)  # [BS]
        neg_scores = query_emb @ negative_item_embs.T  # [BS, num_negatives]
        
        # 4. Softmax with temperature
        all_scores = torch.cat([
            pos_scores.unsqueeze(1),
            neg_scores
        ], dim=1) / self.temperature  # [BS, 1+num_negatives]
        
        # 5. äº¤å‰ç†µ
        labels = torch.zeros(BS, dtype=torch.long)  # æ­£æ ·æœ¬åœ¨ç¬¬0ä½
        loss = F.cross_entropy(all_scores, labels)
        
        return loss
```

### 2. InBatchNegativesSampler

```python
class InBatchNegativesSampler(nn.Module):
    """
    ä»Batchå†…é‡‡æ ·è´Ÿæ ·æœ¬
    
    ä¼˜åŠ¿:
        1. æ— éœ€é¢å¤–é‡‡æ ·å¼€é”€
        2. è´Ÿæ ·æœ¬æ•°éšbatch sizeè‡ªåŠ¨å¢é•¿
        3. åŠ¨æ€è´Ÿæ ·æœ¬ï¼Œæ›´æœ‰æ•ˆ
    
    ç­–ç•¥: 
        - å¯¹äºæ ·æœ¬iï¼ŒBatchå†…å…¶ä»–æ ·æœ¬çš„æ­£æ ·æœ¬éƒ½æ˜¯içš„è´Ÿæ ·æœ¬
        - è‡ªåŠ¨å»é‡ (é¿å…é‡‡æ ·åˆ°ç›¸åŒç‰©å“)
    """
    
    def forward(
        self,
        positive_item_ids,   # [BS]
        positive_item_emb,   # [BS, D]
    ):
        # 1. Batchå†…æ‰€æœ‰æ­£æ ·æœ¬éƒ½å¯ä»¥ä½œä¸ºè´Ÿæ ·æœ¬
        all_candidate_embs = positive_item_emb  # [BS, D]
        
        # 2. å»é‡ (å¯é€‰)
        if self.dedup_embeddings:
            unique_item_ids, inverse_indices = torch.unique(
                positive_item_ids, return_inverse=True
            )
            all_candidate_embs = positive_item_emb[inverse_indices]
        
        # 3. é‡‡æ · (éšæœºé€‰æ‹©num_to_sampleä¸ª)
        # å®é™…å®ç°ä¸­ç›´æ¥ä½¿ç”¨æ‰€æœ‰Batchå†…æ ·æœ¬ä½œä¸ºè´Ÿæ ·æœ¬
        negative_embs = all_candidate_embs  # [BS, D]
        
        return negative_embs
```

### 3. L2NormEmbeddingPostprocessor

```python
class L2NormEmbeddingPostprocessor(nn.Module):
    """
    L2å½’ä¸€åŒ–
    
    ä¸ºä»€ä¹ˆéœ€è¦?
        1. è®©ç›¸ä¼¼åº¦åªå…³æ³¨æ–¹å‘ï¼Œä¸å…³æ³¨é•¿åº¦
        2. æé«˜è®­ç»ƒç¨³å®šæ€§
        3. ä½¿æ‰€æœ‰embeddingåœ¨å•ä½çƒé¢ä¸Š
    
    å…¬å¼:
        x_normalized = x / (||x||â‚‚ + eps)
    """
    
    def forward(self, embeddings):
        # [BS, D] â†’ [BS, D]
        return F.normalize(embeddings, p=2, dim=-1, eps=self.eps)
```

### 4. DotProductSimilarity

```python
class DotProductSimilarity(nn.Module):
    """
    ç‚¹ç§¯ç›¸ä¼¼åº¦
    
    å…¬å¼:
        sim(x, y) = x Â· y = Î£áµ¢ xáµ¢yáµ¢
    
    ç‰¹ç‚¹:
        - å½’ä¸€åŒ–åç­‰ä»·äºä½™å¼¦ç›¸ä¼¼åº¦
        - è®¡ç®—é«˜æ•ˆ
        - GPUå‹å¥½
    """
    
    def forward(self, query_emb, item_embs):
        # query_emb: [BS, D]
        # item_embs: [BS, N, D] or [N, D]
        
        # ç‚¹ç§¯
        scores = torch.matmul(query_emb, item_embs.T)  # [BS, N]
        
        return scores
```

---

## å®é™…ä¾‹å­ï¼šMovieLens-20M Retrieval

### æ•°æ®æµç¤ºä¾‹

```python
# è¾“å…¥æ ·æœ¬
user_id = 1
movie_sequence = [924, 919, 2683, 1584, 1079]  # 5éƒ¨ç”µå½±
rating_sequence = [6, 6, 6, 7, 5]

# è®­ç»ƒç›®æ ‡: æ ¹æ®å‰4éƒ¨ç”µå½± [924, 919, 2683, 1584]
#           é¢„æµ‹ç¬¬5éƒ¨ç”µå½± [1079]

# ======================================
# ç¬¬1æ­¥: Embedding
# ======================================
movie_embs = embedding_table["movie"][[924, 919, 2683, 1584, 1079]]
# shape: [5, 256]

# ======================================
# ç¬¬2æ­¥: HSTU Block
# ======================================
# è¾“å…¥: [embâ‚, embâ‚‚, embâ‚ƒ, embâ‚„, embâ‚…]
# è¾“å‡º: [hâ‚, hâ‚‚, hâ‚ƒ, hâ‚„, hâ‚…]
hstu_output = hstu_block(movie_embs)  # [5, 256]

# ======================================
# ç¬¬3æ­¥: SplitåŒå¡”
# ======================================
# Query Tower: å‰4ä¸ªçš„HSTUè¾“å‡º
query_emb = hstu_output[3]  # hâ‚„, shape: [256]
# å«ä¹‰: åŸºäºå‰4éƒ¨ç”µå½±çš„åå¥½è¡¨ç¤º

# Item Tower: ç¬¬5ä¸ªçš„åŸå§‹embedding
positive_item_emb = embedding_table["movie"][1079]  # [256]
# å«ä¹‰: ç”µå½±1079çš„è¡¨ç¤º

# ======================================
# ç¬¬4æ­¥: è´Ÿæ ·æœ¬é‡‡æ ·
# ======================================
# ä»Batchå†…å…¶ä»–æ ·æœ¬çš„ç›®æ ‡ç‰©å“é‡‡æ ·128ä¸ª
# negative_item_embs: [128, 256]

# ======================================
# ç¬¬5æ­¥: L2å½’ä¸€åŒ–
# ======================================
query_emb = normalize(query_emb)
positive_item_emb = normalize(positive_item_emb)
negative_item_embs = normalize(negative_item_embs)

# ======================================
# ç¬¬6æ­¥: ç›¸ä¼¼åº¦è®¡ç®—
# ======================================
pos_score = query_emb @ positive_item_emb  # æ ‡é‡
neg_scores = query_emb @ negative_item_embs.T  # [128]

all_scores = [pos_score, neg_scores]  # [129]

# ======================================
# ç¬¬7æ­¥: Softmax Loss
# ======================================
# ç›®æ ‡: pos_score > neg_scores
probs = softmax(all_scores / 0.05)
loss = -log(probs[0])  # æœ€å¤§åŒ–æ­£æ ·æœ¬æ¦‚ç‡
```

### æ¨ç†ç¤ºä¾‹

```python
# åœºæ™¯: ç”¨æˆ·1çš„æ–°sessionï¼Œçœ‹è¿‡[924, 919, 2683, 1584]
# ä»»åŠ¡: å¬å›100éƒ¨å¯èƒ½æ„Ÿå…´è¶£çš„ç”µå½±

# ç¬¬1æ­¥: è®¡ç®—query embedding
user_history = [924, 919, 2683, 1584]
query_emb = hstu_block(user_history)  # [256]
query_emb = normalize(query_emb)

# ç¬¬2æ­¥: ä»ç‰©å“åº“æ£€ç´¢ (å‡è®¾æœ‰26744éƒ¨ç”µå½±)
all_movie_embs = embedding_table["movie"]  # [26744, 256]
all_movie_embs = normalize(all_movie_embs)

scores = query_emb @ all_movie_embs.T  # [26744]

# ç¬¬3æ­¥: TopKå¬å›
top100_indices = scores.topk(k=100).indices
recommended_movies = movie_ids[top100_indices]

# ç»“æœ: [1079, 2959, 337, ...]  (å¯èƒ½åŒ…å«çœŸå®çœ‹è¿‡çš„1079)
```

---

## å…³é”®å‚æ•°

| å‚æ•° | é…ç½®ä½ç½® | ç¤ºä¾‹å€¼ | è¯´æ˜ |
|------|----------|--------|------|
| `num_layers` | NetworkArgs | 4 | HSTUå±‚æ•° (Retrievalé€šå¸¸æ¯”Rankingå¤š) |
| `hidden_size` | NetworkArgs | 256 | Embeddingç»´åº¦ (å¿…é¡»ä¸embeddingè¡¨ä¸€è‡´) |
| `num_negatives` | RetrievalArgs | 128 | è´Ÿæ ·æœ¬æ•° (å½±å“è®­ç»ƒè´¨é‡å’Œé€Ÿåº¦) |
| `temperature` | RetrievalArgs | 0.05 | Softmaxæ¸©åº¦ (è¶Šå°è¶Šé™¡å³­) |
| `l2_norm_eps` | RetrievalArgs | 1e-6 | L2å½’ä¸€åŒ–epsilon (æ•°å€¼ç¨³å®šæ€§) |
| `is_causal` | NetworkArgs | True | æ˜¯å¦ä½¿ç”¨å› æœmask |

---

## è®­ç»ƒ vs æ¨ç†

### è®­ç»ƒé˜¶æ®µ

```python
# æ•°æ®: [user_history, target_item]
# æµç¨‹:
#   1. HSTUç¼–ç user_history â†’ query_emb
#   2. æŸ¥è¡¨è·å–target_item_emb (æ­£æ ·æœ¬)
#   3. ä»Batché‡‡æ ·è´Ÿæ ·æœ¬
#   4. è®¡ç®—Sampled Softmax Loss
#   5. åå‘ä¼ æ’­æ›´æ–°å‚æ•°

# ä¼˜åŒ–ç›®æ ‡:
#   max sim(query_emb, positive_item_emb)
#   min sim(query_emb, negative_item_embs)
```

### æ¨ç†é˜¶æ®µ

```python
# ç¦»çº¿é˜¶æ®µ:
#   1. ä¸ºæ‰€æœ‰ç‰©å“æ„å»ºembeddingç´¢å¼•
#   all_item_embs = embedding_table["item"]
#   2. å­˜å…¥å‘é‡æ•°æ®åº“ (Faiss, HNSW)

# åœ¨çº¿é˜¶æ®µ:
#   1. ç”¨æˆ·è¯·æ±‚åˆ°è¾¾
#   2. HSTUç¼–ç user_history â†’ query_emb
#   3. ANNæ£€ç´¢topKæœ€ç›¸ä¼¼ç‰©å“
#   4. è¿”å›å¬å›ç»“æœ (å¯èƒ½è¿›å…¥ç²¾æ’)

# æ€§èƒ½ä¼˜åŒ–:
#   - ä½¿ç”¨GPUåŠ é€ŸHSTUæ¨ç†
#   - ä½¿ç”¨é«˜æ•ˆANNåº“ (Faiss GPU)
#   - Batchæ¨ç† (å¤šä¸ªç”¨æˆ·å¹¶è¡Œ)
```

---

## æ€»ç»“

### æ¨¡å‹ç»“æ„åœ¨å“ªé‡Œå®šä¹‰ï¼Ÿ

1. **æ ¸å¿ƒå®šä¹‰**: `model/retrieval_gr.py` çš„ `RetrievalGR.__init__()`
   ```python
   self._embedding_collection = ShardedEmbedding(...)
   self._hstu_block = HSTUBlock(...)
   self._loss_module = SampledSoftmaxLoss(...)  # â† æ ¸å¿ƒå·®å¼‚
   # æ³¨æ„: æ²¡æœ‰MLP!
   ```

2. **å‰å‘æµç¨‹**: `model/retrieval_gr.py` çš„ `get_logit_and_labels()`
   ```python
   embeddings = self._embedding_collection(batch.features)
   hidden_states = self._hstu_block(embeddings, batch)
   query_emb, positive_item_emb = split_towers(hidden_states)  # â† åŒå¡”
   ```

3. **ç»“æ„é…ç½®**: `movielen_retrieval.gin`
   ```python
   NetworkArgs.num_layers = 4
   NetworkArgs.hidden_size = 256
   RetrievalArgs.num_negatives = 128  # â† Retrievalç‰¹æœ‰
   ```

### å…³é”®æ–‡ä»¶é€ŸæŸ¥

| å†…å®¹ | æ–‡ä»¶è·¯å¾„ | å…³é”®ä»£ç  |
|------|----------|----------|
| **æ¨¡å‹æ€»ä½“å®šä¹‰** | `model/retrieval_gr.py` | `RetrievalGR.__init__()` (46-82è¡Œ) |
| **å‰å‘ä¼ æ’­æµç¨‹** | `model/retrieval_gr.py` | `get_logit_and_labels()` (104-160è¡Œ) |
| **åŒå¡”Split** | `model/retrieval_gr.py` | `triton_split_2D_jagged()` (136-154è¡Œ) |
| **Sampled Softmax Loss** | `modules/sampled_softmax_loss.py` | `SampledSoftmaxLoss` |
| **è´Ÿæ ·æœ¬é‡‡æ ·å™¨** | `modules/negatives_sampler.py` | `InBatchNegativesSampler` |
| **L2å½’ä¸€åŒ–** | `modules/output_postprocessors.py` | `L2NormEmbeddingPostprocessor` |
| **é…ç½®æ–‡ä»¶** | `movielen_retrieval.gin` | ginå‚æ•° |
| **è®­ç»ƒå…¥å£** | `pretrain_gr_retrieval.py` | `main()` |

### Retrieval vs Ranking å¿«é€Ÿå¯¹æ¯”

| ç‰¹æ€§ | Retrieval | Ranking |
|------|-----------|---------|
| è¾“å‡º | Embedding | Logits |
| é¢„æµ‹å¤´ | æ—  | MLP |
| Loss | Sampled Softmax | BCE/CE |
| ç»“æ„ | åŒå¡” | å•å¡” |
| è´Ÿæ ·æœ¬ | In-Batch | æ—  |
| å½’ä¸€åŒ– | L2 Norm | æ—  |
| è¯„ä¼° | NDCG, HR | AUC |

---

**ç°åœ¨æ‚¨åº”è¯¥å®Œå…¨ç†è§£Retrievalå¬å›æ¨¡å‹çš„ç»“æ„äº†ï¼** ğŸ‰

æ ¸å¿ƒè¦ç‚¹ï¼š
1. **æ— MLP**: ç›´æ¥ä½¿ç”¨HSTUè¾“å‡ºçš„embedding
2. **åŒå¡”ç»“æ„**: Query Tower (HSTU) + Item Tower (åŸå§‹embedding)
3. **Sampled Softmax Loss**: å¯¹æ¯”å­¦ä¹ ï¼Œæ‹‰è¿‘æ­£æ ·æœ¬ï¼Œæ¨å¼€è´Ÿæ ·æœ¬
4. **In-Batch Negatives**: é«˜æ•ˆçš„è´Ÿæ ·æœ¬é‡‡æ ·ç­–ç•¥
5. **L2å½’ä¸€åŒ–**: è®©ç›¸ä¼¼åº¦è®¡ç®—æ›´ç¨³å®š

éœ€è¦è¿›ä¸€æ­¥è§£é‡ŠæŸä¸ªéƒ¨åˆ†å—ï¼Ÿä¾‹å¦‚ä¸ºä»€ä¹ˆRetrievaléœ€è¦L2å½’ä¸€åŒ–ï¼ŸğŸ”

