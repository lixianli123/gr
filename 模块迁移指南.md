# ğŸ”§ HSTU + Dynamic Embeddings æ¨¡å—è¿ç§»æŒ‡å—

## ğŸ“‹ ç›®å½•
1. [å¿«é€Ÿæ–¹æ¡ˆ](#å¿«é€Ÿæ–¹æ¡ˆ)
2. [æ ¸å¿ƒæ–‡ä»¶æ¸…å•](#æ ¸å¿ƒæ–‡ä»¶æ¸…å•)
3. [æœ€å°ä¾èµ–](#æœ€å°ä¾èµ–)
4. [è¿ç§»æ­¥éª¤](#è¿ç§»æ­¥éª¤)
5. [ç‹¬ç«‹ä½¿ç”¨ç¤ºä¾‹](#ç‹¬ç«‹ä½¿ç”¨ç¤ºä¾‹)

---

## å¿«é€Ÿæ–¹æ¡ˆ

### æ–¹æ¡ˆA: ç›´æ¥ä½¿ç”¨å·²ç¼–è¯‘çš„æ¨¡å— (æ¨è â­)

**æœ€ç®€å•ï¼åªéœ€3æ­¥ï¼š**

```bash
# 1. ç¼–è¯‘ç”ŸæˆwheelåŒ…
cd /path/to/recsys-examples-main

# ç¼–è¯‘HSTU Attention
cd corelib/hstu
pip install . --no-deps
# ç”Ÿæˆ: hstu_attn-*.whl

# ç¼–è¯‘Dynamic Embeddings
cd ../dynamicemb
pip install . --no-deps
# ç”Ÿæˆ: dynamicemb-*.whl

# 2. æ‹·è´wheelåŒ…åˆ°ä½ çš„é¡¹ç›®
cp corelib/hstu/dist/hstu_attn-*.whl /your/project/
cp corelib/dynamicemb/dist/dynamicemb-*.whl /your/project/

# 3. åœ¨ä½ çš„é¡¹ç›®ä¸­å®‰è£…
cd /your/project/
pip install hstu_attn-*.whl
pip install dynamicemb-*.whl
```

**ä½¿ç”¨**ï¼š
```python
# åœ¨ä½ çš„æ¨¡å‹ä¸­ç›´æ¥å¯¼å…¥
from hstu_attn import hstu_attn_varlen_func
from dynamicemb import DynamicEmbeddingBagCollection

# å°±å¯ä»¥ç”¨äº†ï¼
```

---

### æ–¹æ¡ˆB: åˆ›å»ºç‹¬ç«‹PythonåŒ… (çµæ´»)

åˆ›å»ºä¸€ä¸ªæ–°çš„PythonåŒ…ï¼ŒåŒ…å«è¿™ä¸¤ä¸ªæ¨¡å—çš„æ¥å£å°è£…ã€‚

---

## æ ¸å¿ƒæ–‡ä»¶æ¸…å•

### å¿…éœ€æ–‡ä»¶ (æœ€å°é›†åˆ)

```
your_project/
â”œâ”€â”€ accelerated_modules/          # æ–°å»ºç›®å½•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ hstu_attention.py         # HSTUå°è£…
â”‚   â”œâ”€â”€ dynamic_embedding.py      # Dynamic Embå°è£…
â”‚   â””â”€â”€ compiled/                 # ç¼–è¯‘åçš„åº“
â”‚       â”œâ”€â”€ hstu_attn_2_cuda.so   # HSTU CUDAåº“
â”‚       â””â”€â”€ dynamicemb_extensions.so  # DynamicEmb CUDAåº“
â””â”€â”€ third_party/                  # ä¾èµ–çš„å¤´æ–‡ä»¶ (ç¼–è¯‘æ—¶éœ€è¦)
    â”œâ”€â”€ cutlass/                  # CUTLASSåº“
    â””â”€â”€ HierarchicalKV/           # HKVåº“
```

---

## æœ€å°ä¾èµ–

### Pythonä¾èµ–

```python
# requirements.txt
torch>=2.0.0
einops
packaging
ninja  # ç¼–è¯‘æ—¶éœ€è¦
```

### ç³»ç»Ÿä¾èµ–

```bash
# CUDA
CUDA >= 11.6 (æ¨è 12.1)

# ç¼–è¯‘å·¥å…·
gcc >= 7.5
cmake >= 3.18

# Gitå­æ¨¡å— (ç¼–è¯‘æ—¶éœ€è¦)
CUTLASS (third_party/cutlass)
HierarchicalKV (third_party/HierarchicalKV)
```

### TorchRecä¾èµ– (ä»…Dynamic Embeddings)

```bash
# Dynamic Embeddingséœ€è¦TorchRecæ¥å£
pip install torchrec>=1.2.0
# æˆ–ä½¿ç”¨å…¼å®¹ç‰ˆæœ¬
```

---

## è¿ç§»æ­¥éª¤

### æ­¥éª¤1: æå–ç¼–è¯‘åçš„åº“æ–‡ä»¶

```bash
# 1.1 æ‰¾åˆ°ç¼–è¯‘åçš„.soæ–‡ä»¶
cd /path/to/recsys-examples-main

# HSTU Attentionçš„.so
find . -name "hstu_attn_2_cuda*.so"
# è¾“å‡º: ./corelib/hstu/hstu_attn/hstu_attn_2_cuda.cpython-310-x86_64-linux-gnu.so

# Dynamic Embeddingsçš„.so
find . -name "dynamicemb_extensions*.so"
# è¾“å‡º: ./corelib/dynamicemb/dynamicemb/dynamicemb_extensions.cpython-310-x86_64-linux-gnu.so

# 1.2 æ‹·è´åˆ°ä½ çš„é¡¹ç›®
mkdir -p /your/project/accelerated_modules/compiled
cp corelib/hstu/hstu_attn/hstu_attn_2_cuda*.so /your/project/accelerated_modules/compiled/
cp corelib/dynamicemb/dynamicemb/dynamicemb_extensions*.so /your/project/accelerated_modules/compiled/
```

### æ­¥éª¤2: æå–Pythonæ¥å£æ–‡ä»¶

```bash
# 2.1 HSTU Attentionæ¥å£
cp corelib/hstu/hstu_attn_interface.py /your/project/accelerated_modules/hstu_attention.py

# 2.2 Dynamic Embeddingsæ¥å£
cp -r corelib/dynamicemb/dynamicemb /your/project/accelerated_modules/
```

### æ­¥éª¤3: åˆ›å»ºç»Ÿä¸€æ¥å£

åˆ›å»º `/your/project/accelerated_modules/__init__.py`:

```python
"""
HSTU + Dynamic Embeddings åŠ é€Ÿæ¨¡å—

ä½¿ç”¨æ–¹æ³•:
    from accelerated_modules import HSTUAttention, DynamicEmbedding
"""

import os
import sys

# æ·»åŠ ç¼–è¯‘åº“è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
compiled_dir = os.path.join(current_dir, "compiled")
sys.path.insert(0, compiled_dir)

# å¯¼å…¥HSTU Attention
try:
    from hstu_attn import hstu_attn_varlen_func
    
    class HSTUAttention:
        """HSTU Attentionå°è£…"""
        
        @staticmethod
        def forward(q, k, v, cu_seqlens_q, cu_seqlens_k, 
                   max_seqlen_q, max_seqlen_k, 
                   dropout_p=0.0, softmax_scale=None,
                   causal=False, window_size=(-1, -1),
                   return_softmax=False):
            """
            HSTU Attentionå‰å‘ä¼ æ’­
            
            Args:
                q: Query tensor [total_q, num_heads, head_dim]
                k: Key tensor [total_k, num_heads, head_dim]
                v: Value tensor [total_k, num_heads, head_dim]
                cu_seqlens_q: Queryåºåˆ—åç§» [batch_size + 1]
                cu_seqlens_k: Keyåºåˆ—åç§» [batch_size + 1]
                max_seqlen_q: Queryæœ€å¤§åºåˆ—é•¿åº¦
                max_seqlen_k: Keyæœ€å¤§åºåˆ—é•¿åº¦
                
            Returns:
                output: Attentionè¾“å‡º [total_q, num_heads, head_dim]
            """
            return hstu_attn_varlen_func(
                q, k, v,
                cu_seqlens_q, cu_seqlens_k,
                max_seqlen_q, max_seqlen_k,
                dropout_p=dropout_p,
                softmax_scale=softmax_scale,
                causal=causal,
                window_size=window_size,
                return_softmax=return_softmax,
            )
    
    HSTU_AVAILABLE = True
except ImportError as e:
    print(f"è­¦å‘Š: HSTU Attentionä¸å¯ç”¨: {e}")
    HSTUAttention = None
    HSTU_AVAILABLE = False

# å¯¼å…¥Dynamic Embeddings
try:
    from dynamicemb import (
        DynamicEmbeddingBagCollection,
        DynamicEmbeddingCollection,
        DynamicEmbTableOptions,
    )
    
    DYNAMICEMB_AVAILABLE = True
except ImportError as e:
    print(f"è­¦å‘Š: Dynamic Embeddingsä¸å¯ç”¨: {e}")
    DynamicEmbeddingBagCollection = None
    DynamicEmbeddingCollection = None
    DynamicEmbTableOptions = None
    DYNAMICEMB_AVAILABLE = False

# å¯¼å‡º
__all__ = [
    'HSTUAttention',
    'DynamicEmbeddingBagCollection',
    'DynamicEmbeddingCollection',
    'DynamicEmbTableOptions',
    'HSTU_AVAILABLE',
    'DYNAMICEMB_AVAILABLE',
]

# ç‰ˆæœ¬æ£€æŸ¥
def check_cuda_availability():
    """æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨"""
    import torch
    if not torch.cuda.is_available():
        raise RuntimeError("CUDAä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨åŠ é€Ÿæ¨¡å—")
    
    device_capability = torch.cuda.get_device_capability()
    if device_capability[0] < 8:
        raise RuntimeError(
            f"GPUè®¡ç®—èƒ½åŠ› {device_capability} ä¸æ”¯æŒè¿™äº›åŠ é€Ÿæ¨¡å— (éœ€è¦ >= 8.0)"
        )
    
    print(f"âœ“ CUDAå¯ç”¨: {torch.version.cuda}")
    print(f"âœ“ GPU: {torch.cuda.get_device_name(0)}")
    print(f"âœ“ è®¡ç®—èƒ½åŠ›: {device_capability}")

def print_info():
    """æ‰“å°æ¨¡å—ä¿¡æ¯"""
    print("=" * 50)
    print("åŠ é€Ÿæ¨¡å—çŠ¶æ€:")
    print(f"  HSTU Attention: {'âœ“ å¯ç”¨' if HSTU_AVAILABLE else 'âœ— ä¸å¯ç”¨'}")
    print(f"  Dynamic Embeddings: {'âœ“ å¯ç”¨' if DYNAMICEMB_AVAILABLE else 'âœ— ä¸å¯ç”¨'}")
    print("=" * 50)
```

### æ­¥éª¤4: ä½¿ç”¨ç¤ºä¾‹

åˆ›å»º `/your/project/example_usage.py`:

```python
"""ä½¿ç”¨åŠ é€Ÿæ¨¡å—çš„ç¤ºä¾‹"""

import torch
from accelerated_modules import (
    HSTUAttention,
    DynamicEmbeddingBagCollection,
    check_cuda_availability,
    print_info
)

# æ£€æŸ¥ç¯å¢ƒ
check_cuda_availability()
print_info()

# ============================================
# ç¤ºä¾‹1: ä½¿ç”¨HSTU Attention
# ============================================
print("\nç¤ºä¾‹1: HSTU Attention")

# åˆ›å»ºæµ‹è¯•æ•°æ®
batch_size = 2
num_heads = 4
head_dim = 64
seqlen = 100

# å˜é•¿åºåˆ—æ ¼å¼ (Jagged Tensor)
total_q = batch_size * seqlen
q = torch.randn(total_q, num_heads, head_dim, dtype=torch.bfloat16).cuda()
k = torch.randn(total_q, num_heads, head_dim, dtype=torch.bfloat16).cuda()
v = torch.randn(total_q, num_heads, head_dim, dtype=torch.bfloat16).cuda()

# åºåˆ—åç§» (cumulative sequence lengths)
cu_seqlens = torch.tensor([0, seqlen, 2*seqlen], dtype=torch.int32).cuda()

# HSTU Attentionå‰å‘ä¼ æ’­
output = HSTUAttention.forward(
    q, k, v,
    cu_seqlens_q=cu_seqlens,
    cu_seqlens_k=cu_seqlens,
    max_seqlen_q=seqlen,
    max_seqlen_k=seqlen,
    causal=True,  # å› æœmask
)

print(f"è¾“å…¥shape: {q.shape}")
print(f"è¾“å‡ºshape: {output.shape}")
print(f"è¾“å‡ºdtype: {output.dtype}")
print("âœ“ HSTU Attentionå·¥ä½œæ­£å¸¸")

# ============================================
# ç¤ºä¾‹2: ä½¿ç”¨Dynamic Embeddings
# ============================================
print("\nç¤ºä¾‹2: Dynamic Embeddings")

from torchrec import EmbeddingBagConfig

# å®šä¹‰embeddingé…ç½®
tables = [
    EmbeddingBagConfig(
        name="user_emb",
        embedding_dim=128,
        num_embeddings=1000000,  # 100ä¸‡ç”¨æˆ·
        feature_names=["user_id"],
    ),
    EmbeddingBagConfig(
        name="item_emb",
        embedding_dim=128,
        num_embeddings=10000000,  # 1000ä¸‡ç‰©å“
        feature_names=["item_id"],
    ),
]

# åˆ›å»ºDynamic Embedding Collection
ebc = DynamicEmbeddingBagCollection(
    tables=tables,
    device=torch.device("cuda:0"),
)

print(f"åˆ›å»ºäº† {len(tables)} ä¸ªåŠ¨æ€embeddingè¡¨")
print("âœ“ Dynamic Embeddingså·¥ä½œæ­£å¸¸")

# ============================================
# ç¤ºä¾‹3: é›†æˆåˆ°ä½ çš„æ¨¡å‹
# ============================================
print("\nç¤ºä¾‹3: åœ¨è‡ªå®šä¹‰æ¨¡å‹ä¸­ä½¿ç”¨")

class YourCustomModel(torch.nn.Module):
    """ä½ çš„è‡ªå®šä¹‰æ¨¡å‹"""
    
    def __init__(self, hidden_size=128, num_heads=4):
        super().__init__()
        
        # ä½¿ç”¨Dynamic Embeddings
        self.embeddings = DynamicEmbeddingBagCollection(
            tables=[
                EmbeddingBagConfig(
                    name="features",
                    embedding_dim=hidden_size,
                    num_embeddings=1000000,
                    feature_names=["feature_id"],
                ),
            ],
            device=torch.device("cuda:0"),
        )
        
        # å…¶ä»–å±‚
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        
        # æŠ•å½±å±‚
        self.q_proj = torch.nn.Linear(hidden_size, hidden_size)
        self.k_proj = torch.nn.Linear(hidden_size, hidden_size)
        self.v_proj = torch.nn.Linear(hidden_size, hidden_size)
        self.out_proj = torch.nn.Linear(hidden_size, hidden_size)
    
    def forward(self, feature_ids, cu_seqlens, max_seqlen):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            feature_ids: ç‰¹å¾ID [batch_size, seq_len]
            cu_seqlens: åºåˆ—åç§» [batch_size + 1]
            max_seqlen: æœ€å¤§åºåˆ—é•¿åº¦
        """
        # 1. Embedding lookup
        from torchrec import KeyedJaggedTensor
        kjt = KeyedJaggedTensor.from_offsets_sync(
            keys=["feature_id"],
            values=feature_ids.flatten(),
            offsets=cu_seqlens,
        )
        emb_dict = self.embeddings(kjt)
        embeddings = emb_dict["features"].values()  # [total_tokens, hidden_size]
        
        # 2. æŠ•å½±åˆ°Q, K, V
        q = self.q_proj(embeddings)  # [total_tokens, hidden_size]
        k = self.k_proj(embeddings)
        v = self.v_proj(embeddings)
        
        # Reshapeä¸ºmulti-headæ ¼å¼
        total_tokens = q.shape[0]
        q = q.view(total_tokens, self.num_heads, self.head_dim)
        k = k.view(total_tokens, self.num_heads, self.head_dim)
        v = v.view(total_tokens, self.num_heads, self.head_dim)
        
        # 3. HSTU Attention
        attn_output = HSTUAttention.forward(
            q, k, v,
            cu_seqlens_q=cu_seqlens,
            cu_seqlens_k=cu_seqlens,
            max_seqlen_q=max_seqlen,
            max_seqlen_k=max_seqlen,
            causal=True,
        )
        
        # 4. è¾“å‡ºæŠ•å½±
        attn_output = attn_output.view(total_tokens, self.hidden_size)
        output = self.out_proj(attn_output)
        
        return output

# åˆ›å»ºæ¨¡å‹
model = YourCustomModel().cuda()
print("âœ“ è‡ªå®šä¹‰æ¨¡å‹åˆ›å»ºæˆåŠŸ")

# æµ‹è¯•
feature_ids = torch.randint(0, 1000000, (2, 50)).cuda()  # [batch=2, seq=50]
cu_seqlens = torch.tensor([0, 50, 100], dtype=torch.int32).cuda()
output = model(feature_ids, cu_seqlens, max_seqlen=50)
print(f"æ¨¡å‹è¾“å‡ºshape: {output.shape}")
print("âœ“ æ¨¡å‹å‰å‘ä¼ æ’­æˆåŠŸ")

print("\n" + "=" * 50)
print("æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ä½ å¯ä»¥åœ¨ä½ çš„é¡¹ç›®ä¸­ä½¿ç”¨è¿™äº›åŠ é€Ÿæ¨¡å—äº† ğŸš€")
print("=" * 50)
```

---

## ç‹¬ç«‹ä½¿ç”¨ç¤ºä¾‹

### åœºæ™¯1: åªä½¿ç”¨HSTU Attention

```python
# minimal_hstu_example.py
import torch
from accelerated_modules import HSTUAttention

# ç®€å•å°è£…æˆæ ‡å‡†Attentionæ¥å£
class FastAttention(torch.nn.Module):
    def __init__(self, hidden_size, num_heads):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        
        self.q_proj = torch.nn.Linear(hidden_size, hidden_size)
        self.k_proj = torch.nn.Linear(hidden_size, hidden_size)
        self.v_proj = torch.nn.Linear(hidden_size, hidden_size)
        self.out_proj = torch.nn.Linear(hidden_size, hidden_size)
    
    def forward(self, x, cu_seqlens, max_seqlen):
        """
        x: [total_tokens, hidden_size]
        cu_seqlens: [batch_size + 1]
        max_seqlen: int
        """
        # æŠ•å½±
        q = self.q_proj(x)
        k = self.k_proj(x)
        v = self.v_proj(x)
        
        # Reshape
        total_tokens = x.shape[0]
        q = q.view(total_tokens, self.num_heads, self.head_dim)
        k = k.view(total_tokens, self.num_heads, self.head_dim)
        v = v.view(total_tokens, self.num_heads, self.head_dim)
        
        # HSTU Attention (CUTLASSåŠ é€Ÿ)
        attn_out = HSTUAttention.forward(
            q, k, v, cu_seqlens, cu_seqlens, max_seqlen, max_seqlen
        )
        
        # è¾“å‡ºæŠ•å½±
        attn_out = attn_out.view(total_tokens, self.hidden_size)
        output = self.out_proj(attn_out)
        
        return output

# ä½¿ç”¨
model = FastAttention(hidden_size=128, num_heads=4).cuda()
x = torch.randn(200, 128).cuda()  # 200ä¸ªtoken
cu_seqlens = torch.tensor([0, 100, 200], dtype=torch.int32).cuda()  # 2ä¸ªåºåˆ—
output = model(x, cu_seqlens, max_seqlen=100)
print(f"è¾“å‡ºshape: {output.shape}")  # [200, 128]
```

### åœºæ™¯2: åªä½¿ç”¨Dynamic Embeddings

```python
# minimal_dynamicemb_example.py
import torch
from torchrec import EmbeddingBagConfig, KeyedJaggedTensor
from accelerated_modules import DynamicEmbeddingBagCollection

# åˆ›å»ºè¶…å¤§è§„æ¨¡embedding (1äº¿ç‰©å“)
huge_embedding = DynamicEmbeddingBagCollection(
    tables=[
        EmbeddingBagConfig(
            name="item_emb",
            embedding_dim=256,
            num_embeddings=100_000_000,  # 1äº¿ï¼
            feature_names=["item_id"],
        ),
    ],
    device=torch.device("cuda:0"),
)

# GPUåªç¼“å­˜çƒ­é—¨ç‰©å“ï¼Œå†·é—¨ç‰©å“åœ¨Hostå†…å­˜
# æ€»å†…å­˜: 100M Ã— 256 Ã— 4B = 102GB
# GPUå†…å­˜: åªç”¨10-20GB (çƒ­é—¨ç¼“å­˜)

# æŸ¥è¯¢
item_ids = torch.tensor([1, 2, 3, 12345678, 99999999]).cuda()
kjt = KeyedJaggedTensor.from_lengths_sync(
    keys=["item_id"],
    values=item_ids,
    lengths=torch.tensor([5]),
)
emb_dict = huge_embedding(kjt)
item_embs = emb_dict["item_emb"].values()  # [5, 256]

print(f"æˆåŠŸæŸ¥è¯¢ {len(item_ids)} ä¸ªç‰©å“çš„embedding")
print(f"Embedding shape: {item_embs.shape}")
```

---

## ç®€åŒ–è„šæœ¬

åˆ›å»º `/your/project/setup_accelerated_modules.sh`:

```bash
#!/bin/bash
# ä¸€é”®è®¾ç½®åŠ é€Ÿæ¨¡å—

set -e

echo "======================================"
echo "è®¾ç½®HSTU + Dynamic EmbeddingsåŠ é€Ÿæ¨¡å—"
echo "======================================"

# æºé¡¹ç›®è·¯å¾„
SOURCE_PATH="/path/to/recsys-examples-main"
TARGET_PATH="./accelerated_modules"

# 1. åˆ›å»ºç›®å½•
echo "[1/5] åˆ›å»ºç›®å½•..."
mkdir -p $TARGET_PATH/compiled

# 2. æ‹·è´ç¼–è¯‘åçš„åº“
echo "[2/5] æ‹·è´ç¼–è¯‘åçš„CUDAåº“..."

# HSTU Attention
HSTU_SO=$(find $SOURCE_PATH/corelib/hstu -name "hstu_attn_2_cuda*.so" | head -n1)
if [ -f "$HSTU_SO" ]; then
    cp "$HSTU_SO" $TARGET_PATH/compiled/
    echo "  âœ“ HSTU Attention: $(basename $HSTU_SO)"
else
    echo "  âœ— HSTU Attentionåº“æœªæ‰¾åˆ°ï¼Œè¯·å…ˆç¼–è¯‘"
fi

# Dynamic Embeddings
DYNAMICEMB_SO=$(find $SOURCE_PATH/corelib/dynamicemb -name "dynamicemb_extensions*.so" | head -n1)
if [ -f "$DYNAMICEMB_SO" ]; then
    cp "$DYNAMICEMB_SO" $TARGET_PATH/compiled/
    echo "  âœ“ Dynamic Embeddings: $(basename $DYNAMICEMB_SO)"
else
    echo "  âœ— Dynamic Embeddingsåº“æœªæ‰¾åˆ°ï¼Œè¯·å…ˆç¼–è¯‘"
fi

# 3. æ‹·è´Pythonæ¥å£
echo "[3/5] æ‹·è´Pythonæ¥å£..."
cp $SOURCE_PATH/corelib/hstu/hstu_attn_interface.py $TARGET_PATH/hstu_attention.py
cp -r $SOURCE_PATH/corelib/dynamicemb/dynamicemb $TARGET_PATH/
echo "  âœ“ Pythonæ¥å£å·²æ‹·è´"

# 4. åˆ›å»º__init__.py
echo "[4/5] åˆ›å»ºç»Ÿä¸€æ¥å£..."
cat > $TARGET_PATH/__init__.py << 'EOF'
# è¿™é‡Œæ”¾ä¸Šé¢çš„__init__.pyå†…å®¹
EOF
echo "  âœ“ ç»Ÿä¸€æ¥å£å·²åˆ›å»º"

# 5. éªŒè¯
echo "[5/5] éªŒè¯å®‰è£…..."
python -c "
from accelerated_modules import check_cuda_availability, print_info
check_cuda_availability()
print_info()
" && echo "  âœ“ éªŒè¯æˆåŠŸ" || echo "  âœ— éªŒè¯å¤±è´¥"

echo ""
echo "======================================"
echo "âœ“ è®¾ç½®å®Œæˆï¼"
echo "======================================"
echo ""
echo "ä½¿ç”¨æ–¹æ³•:"
echo "  from accelerated_modules import HSTUAttention, DynamicEmbeddingBagCollection"
echo ""
echo "ç¤ºä¾‹ä»£ç : example_usage.py"
echo ""
```

ä½¿ç”¨è„šæœ¬ï¼š
```bash
# ä¿®æ”¹SOURCE_PATHä¸ºä½ çš„recsys-examplesè·¯å¾„
vim setup_accelerated_modules.sh

# è¿è¡Œ
bash setup_accelerated_modules.sh
```

---

## å¸¸è§é—®é¢˜

### Q1: ç¼–è¯‘åçš„.soæ–‡ä»¶æ‰¾ä¸åˆ°ï¼Ÿ

```bash
# ç¡®ä¿å·²ç»ç¼–è¯‘
cd /path/to/recsys-examples-main

# ç¼–è¯‘HSTU
cd corelib/hstu
pip install .

# ç¼–è¯‘DynamicEmb
cd ../dynamicemb
pip install .

# æŸ¥æ‰¾.soæ–‡ä»¶
find . -name "*.so"
```

### Q2: å¯¼å…¥æ—¶æç¤ºæ‰¾ä¸åˆ°åº“ï¼Ÿ

```python
# æ£€æŸ¥Pythonè·¯å¾„
import sys
print(sys.path)

# æ‰‹åŠ¨æ·»åŠ è·¯å¾„
sys.path.insert(0, '/path/to/accelerated_modules/compiled')
```

### Q3: CUDAç‰ˆæœ¬ä¸åŒ¹é…ï¼Ÿ

```bash
# æ£€æŸ¥PyTorchçš„CUDAç‰ˆæœ¬
python -c "import torch; print(torch.version.cuda)"

# æ£€æŸ¥ç³»ç»ŸCUDAç‰ˆæœ¬
nvcc --version

# é‡æ–°ç¼–è¯‘ï¼Œä½¿ç”¨åŒ¹é…çš„CUDAç‰ˆæœ¬
```

### Q4: åªæƒ³ç”¨å…¶ä¸­ä¸€ä¸ªæ¨¡å—ï¼Ÿ

```python
# åªç”¨HSTU Attention
from accelerated_modules import HSTUAttention
# DynamicEmbeddingBagCollectionä¼šæ˜¯Noneï¼Œä¸å½±å“ä½¿ç”¨

# åªç”¨Dynamic Embeddings
from accelerated_modules import DynamicEmbeddingBagCollection
# HSTUAttentionä¼šæ˜¯Noneï¼Œä¸å½±å“ä½¿ç”¨
```

---

## æ€§èƒ½æç¤º

### ä¼˜åŒ–å»ºè®®

1. **ä½¿ç”¨BF16**
   ```python
   q = q.to(torch.bfloat16)
   k = k.to(torch.bfloat16)
   v = v.to(torch.bfloat16)
   ```

2. **æ‰¹é‡å¤„ç†**
   ```python
   # å¥½: ä¸€æ¬¡å¤„ç†å¤šä¸ªåºåˆ—
   cu_seqlens = [0, 100, 200, 300]  # 3ä¸ªåºåˆ—
   
   # å·®: ä¸€æ¬¡ä¸€ä¸ªåºåˆ—
   for seq in sequences:
       process_one(seq)
   ```

3. **é¢„çƒ­GPU**
   ```python
   # ç¬¬ä¸€æ¬¡è°ƒç”¨ä¼šæ…¢ï¼Œé¢„çƒ­ä¸€ä¸‹
   for _ in range(10):
       _ = HSTUAttention.forward(q, k, v, ...)
   torch.cuda.synchronize()
   # åç»­è°ƒç”¨å°±å¿«äº†
   ```

---

## æ€»ç»“

### æ¨èæ–¹æ¡ˆ

**æœ€ç®€å•**: æ–¹æ¡ˆA - ç›´æ¥ä½¿ç”¨wheelåŒ…
- âœ… 3æ­¥å®Œæˆ
- âœ… ä¸éœ€è¦ä¿®æ”¹ä»£ç 
- âœ… ä¾èµ–ç®¡ç†ç®€å•

**æœ€çµæ´»**: æ–¹æ¡ˆB - åˆ›å»ºç‹¬ç«‹PythonåŒ…
- âœ… å¯ä»¥è‡ªå®šä¹‰æ¥å£
- âœ… ä¾¿äºåˆ†å‘
- âœ… é›†æˆæ–¹ä¾¿

### æ ¸å¿ƒæ–‡ä»¶

åªéœ€è¦è¿™äº›æ–‡ä»¶ï¼š
```
hstu_attn_2_cuda.so           (çº¦2-5GBï¼ŒCUTLASSç¼–è¯‘äº§ç‰©)
dynamicemb_extensions.so      (çº¦50-100MB)
hstu_attn_interface.py        (Pythonæ¥å£)
dynamicemb/ (ç›®å½•)            (Pythonæ¥å£)
```

### ä½¿ç”¨ç¤ºä¾‹

```python
# æœ€ç®€å•çš„ä½¿ç”¨æ–¹å¼
from accelerated_modules import HSTUAttention

output = HSTUAttention.forward(q, k, v, cu_seqlens, cu_seqlens, max_seqlen, max_seqlen)
```

**å°±è¿™ä¹ˆç®€å•ï¼** ğŸš€

