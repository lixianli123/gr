# A100 HSTU è®­ç»ƒä»£ç  - ä½¿ç”¨æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬ä»£ç å·²ç»è¿‡ç®€åŒ–ï¼Œ**ä¸“é—¨é’ˆå¯¹ A100 GPU ä¼˜åŒ–**ã€‚æ‰€æœ‰éA100çš„é€‚é…ä»£ç å·²è¢«åˆ é™¤ã€‚

## âœ… ç³»ç»Ÿè¦æ±‚

- **GPU**: NVIDIA A100 (å¿…é¡»)
- **CUDA**: 11.8+
- **PyTorch**: 2.0+
- **ç¼–è¯‘æ‰©å±•**: `hstu_attn_2_cuda` (CUTLASS A100 kernels)

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æ£€æŸ¥GPU

å¯åŠ¨å‰ç¡®è®¤ä½ ä½¿ç”¨çš„æ˜¯ A100ï¼š
```bash
nvidia-smi
```

### 2. è¿è¡Œ Ranking ä»»åŠ¡

```bash
torchrun --nproc_per_node=1 examples/hstu/pretrain_gr_ranking.py \
    --gin-config-file=examples/hstu/kuairand_1k_ranking.gin
```

### 3. è¿è¡Œ Retrieval ä»»åŠ¡

```bash
torchrun --nproc_per_node=1 examples/hstu/pretrain_gr_retrieval.py \
    --gin-config-file=examples/hstu/movielen_retrieval.gin
```

## ğŸ” GPU è‡ªåŠ¨æ£€æµ‹

ä»£ç ä¼šåœ¨å¯åŠ¨æ—¶è‡ªåŠ¨æ£€æµ‹ GPUï¼š

âœ… **A100 æ£€æµ‹æˆåŠŸ**:
```
[GPU Check] Detected A100 GPU (SM 8.0) - OK
```

âŒ **éA100 æ£€æµ‹å¤±è´¥**:
```
RuntimeError: This code is optimized for A100 GPU (SM 8.0) only. 
Detected SM X.X. Please use an A100 GPU.
```

## âš™ï¸ é…ç½®è¯´æ˜

### å¿…é¡»çš„é…ç½®é¡¹

æ‰€æœ‰ `.gin` é…ç½®æ–‡ä»¶ä¸­å¿…é¡»åŒ…å«ï¼š

```python
# åç«¯é…ç½®ï¼ˆå¿…é¡»æ˜¯ cutlassï¼‰
NetworkArgs.kernel_backend = "cutlass"

# æ•°æ®ç±»å‹ï¼ˆæ¨è bfloat16ï¼‰
NetworkArgs.dtype_str = "bfloat16"

# å…¶ä»–ç½‘ç»œé…ç½®
NetworkArgs.num_layers = 4
NetworkArgs.hidden_size = 256
NetworkArgs.num_attention_heads = 8
NetworkArgs.kv_channels = 32
```

### ä¸æ”¯æŒçš„é…ç½®

ä»¥ä¸‹é…ç½®ä¼šå¯¼è‡´é”™è¯¯ï¼š
```python
# âŒ é”™è¯¯ï¼šä¸æ”¯æŒå…¶ä»–åç«¯
NetworkArgs.kernel_backend = "triton"   # ä¸æ”¯æŒ
NetworkArgs.kernel_backend = "pytorch"  # ä¸æ”¯æŒ
```

## ğŸ“ ä¸»è¦æ–‡ä»¶ç»“æ„

```
examples/hstu/
â”œâ”€â”€ pretrain_gr_ranking.py      # Ranking è®­ç»ƒå…¥å£
â”œâ”€â”€ pretrain_gr_retrieval.py    # Retrieval è®­ç»ƒå…¥å£
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ hstu_attention.py       # HSTU æ³¨æ„åŠ›ï¼ˆä»… A100 CUTLASSï¼‰
â”‚   â”œâ”€â”€ fused_hstu_layer.py     # èåˆå±‚å®ç°
â”‚   â””â”€â”€ native_hstu_layer.py    # åŸç”Ÿå±‚å®ç°
â”œâ”€â”€ ops/
â”‚   â””â”€â”€ fused_hstu_op.py        # èåˆæ“ä½œï¼ˆä»… A100ï¼‰
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ hstu_config.py          # é…ç½®å®šä¹‰
â””â”€â”€ *.gin                       # å„ç§é…ç½®æ–‡ä»¶
```

## ğŸ¯ ä»£ç æ”¹åŠ¨æ€»ç»“

### åˆ é™¤çš„å†…å®¹
- âŒ H100/H200 æ”¯æŒï¼ˆSM 9.0ï¼‰
- âŒ Triton åç«¯
- âŒ PyTorch åç«¯
- âŒ `corelib/hstu/hopper/` ç›®å½•

### ä¿ç•™çš„å†…å®¹
- âœ… A100 CUTLASS å®ç°
- âœ… åŠ¨æ€ Embedding æ”¯æŒ
- âœ… Tensor Parallel æ”¯æŒ
- âœ… æ‰€æœ‰è®­ç»ƒåŠŸèƒ½

## ğŸ› ï¸ å¸¸è§é—®é¢˜

### Q1: æˆ‘æ²¡æœ‰ A100ï¼Œèƒ½ä¸èƒ½è¿è¡Œï¼Ÿ
**A**: ä¸èƒ½ã€‚æ­¤ä»£ç ä¸“é—¨ä¸º A100 ä¼˜åŒ–ï¼Œå…¶ä»– GPU ä¼šæŠ¥é”™é€€å‡ºã€‚

### Q2: èƒ½ä¸èƒ½ç”¨ Triton åç«¯ï¼Ÿ
**A**: ä¸èƒ½ã€‚Triton åç«¯å·²è¢«åˆ é™¤ï¼Œåªæ”¯æŒ CUTLASSã€‚

### Q3: ä¸ºä»€ä¹ˆè¦åˆ é™¤å…¶ä»– GPU çš„æ”¯æŒï¼Ÿ
**A**: ä¸ºäº†ç®€åŒ–ä»£ç ï¼Œå‡å°‘å¤æ‚çš„é€‚é…é€»è¾‘ï¼Œè®©ä»£ç æ›´æ¸…æ™°æ˜“æ‡‚ã€‚

### Q4: å¦‚ä½•æ¢å¤å¤š GPU æ”¯æŒï¼Ÿ
**A**: ä» git å†å²æ¢å¤åˆ°é‡æ„å‰çš„ç‰ˆæœ¬ã€‚

## ğŸ“Š æ€§èƒ½ç‰¹æ€§

- âœ… **CUTLASS ä¼˜åŒ–**: é’ˆå¯¹ A100 æ·±åº¦ä¼˜åŒ–çš„ CUDA kernels
- âœ… **Tensor Cores**: åˆ©ç”¨ A100 çš„ç¬¬ä¸‰ä»£ Tensor Cores
- âœ… **èåˆæ“ä½œ**: LayerNorm + Linear + Attention èåˆ
- âœ… **å˜é•¿åºåˆ—**: é«˜æ•ˆå¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—

## ğŸ“ é…ç½®æ–‡ä»¶ç¤ºä¾‹

### Ranking ä»»åŠ¡é…ç½®
```python
# kuairand_1k_ranking.gin

# æ•°æ®é›†é…ç½®
DatasetArgs.dataset_name = "kuairand-1k"
DatasetArgs.max_sequence_length = 50

# ç½‘ç»œé…ç½®
NetworkArgs.kernel_backend = "cutlass"
NetworkArgs.dtype_str = "bfloat16"
NetworkArgs.num_layers = 4
NetworkArgs.hidden_size = 256

# è®­ç»ƒé…ç½®
TrainerArgs.train_batch_size = 128
TrainerArgs.eval_batch_size = 256
TrainerArgs.max_train_iters = 10000
```

### Retrieval ä»»åŠ¡é…ç½®
```python
# movielen_retrieval.gin

# æ•°æ®é›†é…ç½®
DatasetArgs.dataset_name = "ml-1m"
DatasetArgs.max_sequence_length = 100

# ç½‘ç»œé…ç½®
NetworkArgs.kernel_backend = "cutlass"
NetworkArgs.dtype_str = "bfloat16"
NetworkArgs.num_layers = 6
NetworkArgs.hidden_size = 512

# è®­ç»ƒé…ç½®
TrainerArgs.train_batch_size = 64
TrainerArgs.eval_batch_size = 128
```

## ğŸ”§ è°ƒè¯•æŠ€å·§

### æŸ¥çœ‹ GPU ä¿¡æ¯
```bash
python -c "import torch; print(f'GPU: {torch.cuda.get_device_name(0)}'); \
           print(f'SM: {torch.cuda.get_device_properties(0).major}.{torch.cuda.get_device_properties(0).minor}')"
```

### æµ‹è¯• GPU æ£€æµ‹
```python
from modules.hstu_attention import check_a100_gpu
check_a100_gpu()  # ä¼šæ‰“å°æˆ–æŠ¥é”™
```

### éªŒè¯ CUTLASS æ‰©å±•
```python
import hstu_attn_2_cuda
print("CUTLASS extension loaded successfully!")
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- `REFACTORING_SUMMARY.md` - è¯¦ç»†çš„é‡æ„è¯´æ˜
- `examples/hstu/README.md` - HSTU æ¨¡å‹è¯´æ˜
- `corelib/dynamicemb/README.md` - åŠ¨æ€ Embedding è¯´æ˜

## ğŸ’¡ æœ€ä½³å®è·µ

1. **å¯åŠ¨å‰æ£€æŸ¥**: ç¡®è®¤ GPU æ˜¯ A100
2. **é…ç½®éªŒè¯**: ç¡®ä¿ `kernel_backend = "cutlass"`
3. **å†…å­˜ç›‘æ§**: A100 æœ‰ 40GB/80GB æ˜¾å­˜ï¼Œæ³¨æ„ batch size
4. **æ··åˆç²¾åº¦**: ä½¿ç”¨ `bfloat16` è·å¾—æœ€ä½³æ€§èƒ½
5. **Tensor Parallel**: å•å¡ç”¨ `tensor_model_parallel_size=1`

## âš ï¸ é‡è¦æç¤º

- æ­¤ä»£ç **ä»…æ”¯æŒ A100 GPU**
- å¿…é¡»ä½¿ç”¨ **CUTLASS åç«¯**
- é A100 è¿è¡Œä¼š**ç«‹å³æŠ¥é”™**
- é…ç½®é”™è¯¯çš„åç«¯ä¼š**ç«‹å³æŠ¥é”™**

---

**æœ€åæ›´æ–°**: 2025-11-02

