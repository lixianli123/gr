# ğŸš€ A100 GPU + HSTU è®­ç»ƒç¯å¢ƒä»0é…ç½®å®Œæ•´æ•™ç¨‹

## ğŸ“‹ ç›®å½•

1. [ç¡¬ä»¶å’Œç³»ç»Ÿæ£€æŸ¥](#1-ç¡¬ä»¶å’Œç³»ç»Ÿæ£€æŸ¥)
2. [CUDAå·¥å…·é“¾å®‰è£…](#2-cudaå·¥å…·é“¾å®‰è£…)
3. [åˆ›å»ºCondaç¯å¢ƒ](#3-åˆ›å»ºcondaç¯å¢ƒ)
4. [å®‰è£…PyTorch](#4-å®‰è£…pytorch)
5. [ç¼–è¯‘ä¾èµ–åº“](#5-ç¼–è¯‘ä¾èµ–åº“)
6. [ç¼–è¯‘CUDAåŠ é€Ÿæ¨¡å—](#6-ç¼–è¯‘cudaåŠ é€Ÿæ¨¡å—)
7. [éªŒè¯å®‰è£…](#7-éªŒè¯å®‰è£…)
8. [æ•°æ®å‡†å¤‡](#8-æ•°æ®å‡†å¤‡)
9. [è¿è¡Œè®­ç»ƒ](#9-è¿è¡Œè®­ç»ƒ)
10. [å¸¸è§é—®é¢˜æ’æŸ¥](#10-å¸¸è§é—®é¢˜æ’æŸ¥)

---

## 1. ç¡¬ä»¶å’Œç³»ç»Ÿæ£€æŸ¥

### 1.1 æ£€æŸ¥GPUå‹å·å’Œæ¶æ„

```bash
# æŸ¥çœ‹GPUä¿¡æ¯
nvidia-smi

# æŸ¥çœ‹è¯¦ç»†çš„GPUæ¶æ„ä¿¡æ¯
nvidia-smi --query-gpu=gpu_name,compute_cap --format=csv

# æŸ¥çœ‹CUDAé©±åŠ¨ç‰ˆæœ¬
nvidia-smi | grep "CUDA Version"
```

**æœŸæœ›è¾“å‡º**ï¼š
```
GPU Name: NVIDIA A100-SXM4-40GB (æˆ– A100-PCIE-40GB / A100-80GB)
Compute Capability: 8.0
CUDA Version: 12.x æˆ–æ›´é«˜
```

**å…³é”®ä¿¡æ¯**ï¼š
- **A100 GPU** çš„è®¡ç®—èƒ½åŠ›ä¸º **SM 8.0** (Compute Capability 8.0)
- HSTU CUTLASS å†…æ ¸ä¸“é—¨ä¸º SM 8.0 ç¼–è¯‘
- é©±åŠ¨ç‰ˆæœ¬éœ€è¦æ”¯æŒ CUDA 11.6+ (æ¨è 12.x)

### 1.2 æ£€æŸ¥ç³»ç»Ÿç¯å¢ƒ

```bash
# æŸ¥çœ‹æ“ä½œç³»ç»Ÿ
cat /etc/os-release

# æŸ¥çœ‹å†…æ ¸ç‰ˆæœ¬
uname -r

# æŸ¥çœ‹CPUæ ¸å¿ƒæ•°
nproc

# æŸ¥çœ‹å¯ç”¨å†…å­˜
free -h

# æŸ¥çœ‹ç£ç›˜ç©ºé—´
df -h
```

**æ¨èé…ç½®**ï¼š
- OS: Ubuntu 20.04/22.04 æˆ– CentOS 7/8
- RAM: è‡³å°‘ 64GB (ç¼–è¯‘æ—¶éœ€è¦å¤§é‡å†…å­˜)
- ç£ç›˜: è‡³å°‘ 100GB å¯ç”¨ç©ºé—´
- CPU: 16+ æ ¸å¿ƒ (ç¼–è¯‘é€Ÿåº¦æ›´å¿«)

---

## 2. CUDAå·¥å…·é“¾å®‰è£…

### 2.1 æ£€æŸ¥ç°æœ‰CUDA

```bash
# æ£€æŸ¥æ˜¯å¦å·²å®‰è£…CUDA
nvcc --version

# æ£€æŸ¥CUDAè·¯å¾„
echo $CUDA_HOME
echo $PATH | grep cuda
```

### 2.2 å®‰è£…CUDA Toolkit (å¦‚æœæœªå®‰è£…)

**æ¨èç‰ˆæœ¬**ï¼šCUDA 12.1+ (å…¼å®¹æ€§å¥½ï¼Œæ€§èƒ½ä¼˜)

```bash
# æ–¹æ³•1: ä½¿ç”¨å®˜æ–¹å®‰è£…è„šæœ¬ (æ¨è)
wget https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run
sudo sh cuda_12.1.0_530.30.02_linux.run

# æ–¹æ³•2: ä½¿ç”¨åŒ…ç®¡ç†å™¨ (Ubuntu)
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.0-1_all.deb
sudo dpkg -i cuda-keyring_1.0-1_all.deb
sudo apt-get update
sudo apt-get -y install cuda-toolkit-12-1

# æ–¹æ³•3: ä½¿ç”¨åŒ…ç®¡ç†å™¨ (CentOS/RHEL)
sudo yum-config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/cuda-rhel8.repo
sudo yum clean all
sudo yum -y install cuda-toolkit-12-1
```

### 2.3 é…ç½®ç¯å¢ƒå˜é‡

```bash
# ç¼–è¾‘ ~/.bashrc
vim ~/.bashrc

# æ·»åŠ ä»¥ä¸‹å†…å®¹ (æ ¹æ®å®é™…CUDAå®‰è£…è·¯å¾„è°ƒæ•´)
export CUDA_HOME=/usr/local/cuda-12.1
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH

# ä½¿ç¯å¢ƒå˜é‡ç”Ÿæ•ˆ
source ~/.bashrc

# éªŒè¯CUDAå®‰è£…
nvcc --version
```

**æœŸæœ›è¾“å‡º**ï¼š
```
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2023 NVIDIA Corporation
Built on Mon_Apr__3_17:16:06_PDT_2023
Cuda compilation tools, release 12.1, V12.1.105
```

### 2.4 å®‰è£…ç¼–è¯‘å·¥å…·

```bash
# Ubuntu
sudo apt-get update
sudo apt-get install -y build-essential cmake git ninja-build

# CentOS/RHEL
sudo yum groupinstall -y "Development Tools"
sudo yum install -y cmake3 git ninja-build

# éªŒè¯å·¥å…·ç‰ˆæœ¬
gcc --version   # éœ€è¦ >= 7.5
g++ --version   # éœ€è¦ >= 7.5
cmake --version # éœ€è¦ >= 3.18
ninja --version
```

---

## 3. åˆ›å»ºCondaç¯å¢ƒ

### 3.1 å®‰è£…Miniconda (å¦‚æœæœªå®‰è£…)

```bash
# ä¸‹è½½Minicondaå®‰è£…è„šæœ¬
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh

# å®‰è£…
bash Miniconda3-latest-Linux-x86_64.sh

# æŒ‰ç…§æç¤ºå®Œæˆå®‰è£…ï¼Œç„¶åé‡æ–°åŠ è½½shell
source ~/.bashrc

# éªŒè¯å®‰è£…
conda --version
```

### 3.2 åˆ›å»ºPythonç¯å¢ƒ

```bash
# åˆ›å»ºPython 3.10ç¯å¢ƒ (æ¨èç‰ˆæœ¬)
conda create -n hstu_a100 python=3.10 -y

# æ¿€æ´»ç¯å¢ƒ
conda activate hstu_a100

# éªŒè¯Pythonç‰ˆæœ¬
python --version  # åº”è¯¥æ˜¾ç¤º Python 3.10.x
```

**ä¸ºä»€ä¹ˆé€‰æ‹©Python 3.10ï¼Ÿ**
- TorchRecã€Megatron-Core å…¼å®¹æ€§æœ€å¥½
- PyTorch 2.x å®Œå…¨æ”¯æŒ
- é¿å… Python 3.12+ çš„å…¼å®¹æ€§é—®é¢˜

---

## 4. å®‰è£…PyTorch

### 4.1 ç¡®å®šPyTorchç‰ˆæœ¬

```bash
# æ£€æŸ¥CUDAç‰ˆæœ¬
nvcc --version | grep "release"

# æ ¹æ®CUDAç‰ˆæœ¬é€‰æ‹©PyTorch
# CUDA 12.1 â†’ PyTorch 2.1+ with CUDA 12.1
# CUDA 11.8 â†’ PyTorch 2.0+ with CUDA 11.8
```

### 4.2 å®‰è£…PyTorch (CUDA 12.1)

```bash
# æ–¹æ³•1: ä½¿ç”¨conda (æ¨èï¼Œè‡ªåŠ¨å¤„ç†ä¾èµ–)
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia -y

# æ–¹æ³•2: ä½¿ç”¨pip (å¦‚æœcondaå®‰è£…å¤±è´¥)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# æ–¹æ³•3: å®‰è£…PyTorch 2.4+ (æœ€æ–°ç¨³å®šç‰ˆ)
pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121
```

### 4.3 éªŒè¯PyTorchå’ŒCUDA

```bash
# è¿›å…¥Pythonäº¤äº’å¼ç¯å¢ƒ
python

# åœ¨Pythonä¸­æ‰§è¡Œä»¥ä¸‹ä»£ç 
import torch
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
print(f"å½“å‰GPU: {torch.cuda.get_device_name(0)}")
print(f"è®¡ç®—èƒ½åŠ›: {torch.cuda.get_device_capability(0)}")

# æµ‹è¯•CUDAè®¡ç®—
x = torch.rand(5, 3).cuda()
print(f"å¼ é‡åœ¨GPUä¸Š: {x.is_cuda}")
print(x)

# é€€å‡ºPython
exit()
```

**æœŸæœ›è¾“å‡º**ï¼š
```
PyTorchç‰ˆæœ¬: 2.4.0+cu121
CUDAå¯ç”¨: True
CUDAç‰ˆæœ¬: 12.1
GPUæ•°é‡: 1 (æˆ–æ›´å¤š)
å½“å‰GPU: NVIDIA A100-SXM4-40GB
è®¡ç®—èƒ½åŠ›: (8, 0)  â† å…³é”®ï¼å¿…é¡»æ˜¯ (8, 0)
å¼ é‡åœ¨GPUä¸Š: True
```

---

## 5. ç¼–è¯‘ä¾èµ–åº“

### 5.1 å®‰è£…åŸºç¡€Pythonä¾èµ–

```bash
# æ¿€æ´»ç¯å¢ƒ
conda activate hstu_a100

# å®‰è£…åŸºç¡€å·¥å…·
pip install --upgrade pip setuptools wheel

# å®‰è£…ç¼–è¯‘ä¾èµ–
pip install ninja psutil packaging einops
```

### 5.2 ç¼–è¯‘å®‰è£…FBGEMM_GPU

FBGEMM_GPU æ˜¯ TorchRec çš„æ ¸å¿ƒä¾èµ–ï¼Œæä¾›é«˜æ•ˆçš„åµŒå…¥è¡¨æ“ä½œã€‚

```bash
# 1. å®‰è£…ç¼–è¯‘ä¾èµ–
pip install --no-cache setuptools==69.5.1 setuptools-git-versioning scikit-build

# 2. å…‹éš†FBGEMMä»“åº“
cd ~
git clone --recursive -b main https://github.com/pytorch/FBGEMM.git fbgemm
cd fbgemm/fbgemm_gpu

# 3. åˆ‡æ¢åˆ°å…¼å®¹çš„æäº¤ç‰ˆæœ¬
git checkout 642ccb980d05aa1be00ccd131c5991b0914e2e64

# 4. ç¼–è¯‘å¹¶å®‰è£… (åªä¸ºA100çš„SM 8.0ç¼–è¯‘)
# TORCH_CUDA_ARCH_LIST="8.0" è¡¨ç¤ºåªç¼–è¯‘ Compute Capability 8.0
python setup.py bdist_wheel --package_variant=cuda -DTORCH_CUDA_ARCH_LIST="8.0"
python setup.py install --package_variant=cuda -DTORCH_CUDA_ARCH_LIST="8.0"

# 5. éªŒè¯å®‰è£…
python -c 'import fbgemm_gpu; print(f"FBGEMM_GPUç‰ˆæœ¬: {fbgemm_gpu.__version__}")'
```

**ç¼–è¯‘æ—¶é—´**: çº¦ 10-30 åˆ†é’Ÿ (å–å†³äºCPUæ€§èƒ½å’Œå†…å­˜)

**å†…å­˜éœ€æ±‚**: è‡³å°‘ 16GB RAM

**å¸¸è§é”™è¯¯**:
- å¦‚æœå‡ºç° `g++: internal compiler error: Killed` â†’ å†…å­˜ä¸è¶³ï¼Œå‡å°‘å¹¶è¡Œç¼–è¯‘æ•°
  ```bash
  MAX_JOBS=2 python setup.py install --package_variant=cuda -DTORCH_CUDA_ARCH_LIST="8.0"
  ```

### 5.3 å®‰è£…TorchRec

```bash
# 1. å®‰è£…TorchRecçš„Pythonä¾èµ–
pip install --no-deps tensordict orjson

# 2. å…‹éš†TorchRecä»“åº“
cd ~
git clone --recursive -b main https://github.com/pytorch/torchrec.git torchrec
cd torchrec

# 3. åˆ‡æ¢åˆ°å…¼å®¹ç‰ˆæœ¬
git checkout 6aaf1fa72e884642f39c49ef232162fa3772055e

# 4. å®‰è£…TorchRec (ä½¿ç”¨--no-depsé¿å…ä¾èµ–å†²çª)
pip install --no-deps .

# 5. éªŒè¯å®‰è£…
python -c 'import torchrec; print(f"TorchRecç‰ˆæœ¬: {torchrec.__version__}")'
```

### 5.4 å®‰è£…Megatron-Core

```bash
# 1. å…‹éš†Megatron-LMä»“åº“
cd ~
git clone -b core_r0.9.0 https://github.com/NVIDIA/Megatron-LM.git megatron-lm

# 2. å®‰è£…Megatron-Core
cd megatron-lm
pip install -e .

# 3. éªŒè¯å®‰è£…
python -c 'import megatron; from megatron.core import parallel_state; print("Megatron-Coreå®‰è£…æˆåŠŸ")'
```

### 5.5 å®‰è£…å…¶ä»–Pythonä¾èµ–

```bash
# å®‰è£…è®­ç»ƒæ‰€éœ€çš„å…¶ä»–åº“
pip install torchx gin-config torchmetrics==1.0.3 typing-extensions iopath

# éªŒè¯æ‰€æœ‰å…³é”®ä¾èµ–
python -c '
import torch
import fbgemm_gpu
import torchrec
import megatron
import gin
import torchmetrics
print("æ‰€æœ‰åŸºç¡€ä¾èµ–å®‰è£…æˆåŠŸï¼")
'
```

---

## 6. ç¼–è¯‘CUDAåŠ é€Ÿæ¨¡å—

è¿™æ˜¯æœ€å…³é”®çš„éƒ¨åˆ†ï¼éœ€è¦ç¼–è¯‘3ä¸ªCUDAåŠ é€Ÿæ¨¡å—ã€‚

### 6.1 åˆå§‹åŒ–Gitå­æ¨¡å—

```bash
# è¿›å…¥é¡¹ç›®æ ¹ç›®å½•
cd /path/to/recsys-examples-main

# åˆå§‹åŒ–CUTLASSå­æ¨¡å— (HSTU attentionçš„CUDAå†…æ ¸ä¾èµ–)
git submodule update --init third_party/cutlass

# åˆå§‹åŒ–HierarchicalKVå­æ¨¡å— (åŠ¨æ€embeddingçš„å“ˆå¸Œè¡¨ä¾èµ–)
git submodule update --init third_party/HierarchicalKV

# éªŒè¯å­æ¨¡å—
ls -la third_party/cutlass/include/  # åº”è¯¥æœ‰å¾ˆå¤šå¤´æ–‡ä»¶
ls -la third_party/HierarchicalKV/include/  # åº”è¯¥æœ‰HierarchicalKVçš„å¤´æ–‡ä»¶
```

**å…³é”®è¯´æ˜**ï¼š
- **CUTLASS**: NVIDIAçš„CUDAæ¨¡æ¿åº“ï¼Œæä¾›é«˜æ€§èƒ½çš„çŸ©é˜µè¿ç®—å†…æ ¸
- **HierarchicalKV**: NVIDIA Merlinçš„å“ˆå¸Œè¡¨åº“ï¼Œæ”¯æŒGPU+ä¸»æœºå†…å­˜çš„åŠ¨æ€åµŒå…¥

### 6.2 ç¼–è¯‘HSTU Attention (CUTLASSå†…æ ¸)

```bash
# è¿›å…¥HSTU attentionç›®å½•
cd corelib/hstu

# è®¾ç½®ç¼–è¯‘é€‰é¡¹ (åªç¼–è¯‘A100éœ€è¦çš„åŠŸèƒ½ï¼ŒåŠ å¿«ç¼–è¯‘é€Ÿåº¦)
export HSTU_DISABLE_86OR89=TRUE        # ç¦ç”¨SM 8.9 (ä¸æ˜¯A100)
export HSTU_DISABLE_ARBITRARY=TRUE     # ç¦ç”¨ä»»æ„mask (ä¸éœ€è¦)
export HSTU_DISABLE_LOCAL=TRUE         # ç¦ç”¨å±€éƒ¨mask (ä¸éœ€è¦)
export HSTU_DISABLE_RAB=TRUE           # ç¦ç”¨ç›¸å¯¹ä½ç½®åç½® (ä¸éœ€è¦)
export HSTU_DISABLE_DRAB=TRUE          # ç¦ç”¨åŠ¨æ€ç›¸å¯¹ä½ç½®åç½® (ä¸éœ€è¦)
export NVCC_THREADS=4                  # NVCCå¹¶è¡Œç¼–è¯‘çº¿ç¨‹æ•°

# ç¼–è¯‘å¹¶å®‰è£… (ä¼šè‡ªåŠ¨ç¼–è¯‘CUTLASSå†…æ ¸)
pip install .

# éªŒè¯å®‰è£…
python -c 'import hstu_attn; print("HSTU Attention (CUTLASS) å®‰è£…æˆåŠŸ")'
```

**ç¼–è¯‘æ—¶é—´**: çº¦ 30-60 åˆ†é’Ÿ (ç”Ÿæˆæ•°ç™¾ä¸ªCUDAæºæ–‡ä»¶å¹¶ç¼–è¯‘)

**å†…å­˜éœ€æ±‚**: è‡³å°‘ 32GB RAM

**ç¼–è¯‘åŸç†**ï¼š
1. `setup.py` ä¼šæ ¹æ® `HEAD_DIMENSIONS`ã€`DTYPE`ã€`MASK` ç­‰ç»„åˆï¼Œè‡ªåŠ¨ç”Ÿæˆæ•°ç™¾ä¸ª `.cu` æ–‡ä»¶
2. æ¯ä¸ª `.cu` æ–‡ä»¶åŒ…å«ç‰¹å®šé…ç½®çš„ CUTLASS æ¨¡æ¿å®ä¾‹åŒ–
3. ç¼–è¯‘å™¨ä¼šä¸º SM 8.0 æ¶æ„ç”Ÿæˆä¼˜åŒ–çš„ PTX å’Œ SASS ä»£ç 

**è°ƒè¯•ç¼–è¯‘é—®é¢˜**ï¼š
```bash
# å¦‚æœç¼–è¯‘å¤±è´¥ï¼Œå°è¯•é€æ­¥è°ƒè¯•
# 1. æ£€æŸ¥CUTLASSæ˜¯å¦æ­£ç¡®åˆå§‹åŒ–
ls -la ../../third_party/cutlass/include/cutlass/

# 2. æ‰‹åŠ¨è§¦å‘æºæ–‡ä»¶ç”Ÿæˆ
python -c "from setup import generate_cuda_sources; generate_cuda_sources()"

# 3. æŸ¥çœ‹ç”Ÿæˆçš„CUDAæ–‡ä»¶
ls -la csrc/hstu_attn/src/generated/

# 4. å‡å°‘å¹¶è¡Œç¼–è¯‘æ•° (é¿å…å†…å­˜ä¸è¶³)
MAX_JOBS=2 pip install .

# 5. å¯ç”¨è¯¦ç»†è¾“å‡º
VERBOSE=1 pip install .
```

### 6.3 ç¼–è¯‘Dynamic Embeddings

```bash
# è¿›å…¥dynamicembç›®å½•
cd ../dynamicemb  # ä»corelib/hstuåˆ°corelib/dynamicemb

# ç¡®ä¿HierarchicalKVå·²åˆå§‹åŒ–
ls -la ../../third_party/HierarchicalKV/include/

# ç¼–è¯‘å¹¶å®‰è£…
python setup.py install

# éªŒè¯å®‰è£…
python -c 'import dynamicemb; print(f"DynamicEmbç‰ˆæœ¬: {dynamicemb.__version__}")'
```

**ç¼–è¯‘æ—¶é—´**: çº¦ 5-15 åˆ†é’Ÿ

### 6.4 ç¼–è¯‘HSTUè®­ç»ƒè¾…åŠ©ç®—å­

```bash
# è¿›å…¥examples/hstuç›®å½•
cd ../../examples/hstu

# ç¼–è¯‘å¹¶å®‰è£… (åŒ…å«jagged tensoræ“ä½œå’Œpaged kvcache)
python setup.py install

# éªŒè¯å®‰è£…
python -c 'import hstu_cuda_ops; import paged_kvcache_ops; print("HSTUè®­ç»ƒç®—å­å®‰è£…æˆåŠŸ")'
```

**ç¼–è¯‘æ—¶é—´**: çº¦ 2-5 åˆ†é’Ÿ

### 6.5 å®Œæ•´éªŒè¯

```bash
# è¿è¡Œå®Œæ•´éªŒè¯è„šæœ¬
python -c '
import torch
print(f"âœ“ PyTorch {torch.__version__}")
print(f"âœ“ CUDA {torch.version.cuda}")
print(f"âœ“ GPU: {torch.cuda.get_device_name(0)}")
print(f"âœ“ Compute Capability: {torch.cuda.get_device_capability(0)}")

import fbgemm_gpu
print(f"âœ“ FBGEMM_GPU {fbgemm_gpu.__version__}")

import torchrec
print(f"âœ“ TorchRec {torchrec.__version__}")

import megatron
print(f"âœ“ Megatron-Core")

import hstu_attn
print(f"âœ“ HSTU Attention (CUTLASS)")

import dynamicemb
print(f"âœ“ DynamicEmb {dynamicemb.__version__}")

import hstu_cuda_ops
import paged_kvcache_ops
print(f"âœ“ HSTU CUDA Ops")

print("\nğŸ‰ æ‰€æœ‰CUDAåŠ é€Ÿæ¨¡å—ç¼–è¯‘æˆåŠŸï¼")
'
```

---

## 7. éªŒè¯å®‰è£…

### 7.1 éªŒè¯A100 GPUæ£€æµ‹

```bash
cd /path/to/recsys-examples-main/examples/hstu

python -c '
from modules.hstu_attention import check_a100_gpu
try:
    check_a100_gpu()
    print("âœ“ A100 GPUæ£€æµ‹é€šè¿‡")
except RuntimeError as e:
    print(f"âœ— GPUæ£€æµ‹å¤±è´¥: {e}")
'
```

### 7.2 æµ‹è¯•HSTU Attentionå‰å‘ä¼ æ’­

```bash
python -c '
import torch
from hstu_attn import hstu_attn_varlen_func

# åˆ›å»ºæµ‹è¯•æ•°æ®
batch_size = 2
nheads = 8
headdim = 64
seqlen = 100

q = torch.randn(batch_size * seqlen, nheads, headdim, dtype=torch.float16).cuda()
k = torch.randn(batch_size * seqlen, nheads, headdim, dtype=torch.float16).cuda()
v = torch.randn(batch_size * seqlen, nheads, headdim, dtype=torch.float16).cuda()
cu_seqlens = torch.tensor([0, seqlen, 2*seqlen], dtype=torch.int32).cuda()

# æ‰§è¡Œattention
out = hstu_attn_varlen_func(
    q, k, v,
    cu_seqlens, cu_seqlens,
    seqlen, seqlen
)

print(f"âœ“ HSTU Attentionæµ‹è¯•æˆåŠŸ")
print(f"  è¾“å…¥shape: {q.shape}")
print(f"  è¾“å‡ºshape: {out.shape}")
print(f"  è¾“å‡ºdtype: {out.dtype}")
'
```

### 7.3 æµ‹è¯•Dynamic Embeddings

```bash
python -c '
import torch
from dynamicemb import DynamicEmbeddingBagCollection
from torchrec import EmbeddingBagConfig

# åˆ›å»ºåŠ¨æ€embeddingé…ç½®
tables = [
    EmbeddingBagConfig(
        name="user_emb",
        embedding_dim=128,
        num_embeddings=1000000,  # 100ä¸‡ç”¨æˆ·
        feature_names=["user_id"],
    ),
]

# åˆ›å»ºDynamicEmbeddingBagCollection
ebc = DynamicEmbeddingBagCollection(
    tables=tables,
    device=torch.device("cuda:0"),
)

print(f"âœ“ Dynamic Embeddingsæµ‹è¯•æˆåŠŸ")
print(f"  è¡¨æ•°é‡: {len(tables)}")
print(f"  è®¾å¤‡: cuda:0")
'
```

---

## 8. æ•°æ®å‡†å¤‡

### 8.1 ä¸‹è½½å’Œå¤„ç†æ•°æ®

```bash
# è¿›å…¥examples/hstuç›®å½•
cd /path/to/recsys-examples-main/examples/hstu

# åˆ›å»ºæ•°æ®ç›®å½•
mkdir -p ./tmp_data

# å¤„ç†MovieLens-20Mæ•°æ®é›†
python preprocessor.py --dataset_name ml-20m

# æŸ¥çœ‹ç”Ÿæˆçš„æ•°æ®
ls -lh ./tmp_data/ml-20m/
```

**æœŸæœ›è¾“å‡º**ï¼š
```
tmp_data/ml-20m/
â”œâ”€â”€ ratings.csv           # åŸå§‹è¯„åˆ†æ•°æ® (çº¦500MB)
â”œâ”€â”€ movies.csv            # ç”µå½±å…ƒæ•°æ®
â”œâ”€â”€ tags.csv              # æ ‡ç­¾æ•°æ®
â”œâ”€â”€ links.csv             # é“¾æ¥æ•°æ®
â”œâ”€â”€ genome-tags.csv       # åŸºå› ç»„æ ‡ç­¾
â”œâ”€â”€ genome-scores.csv     # åŸºå› ç»„è¯„åˆ†
â””â”€â”€ processed_seqs.csv    # å¤„ç†åçš„åºåˆ—æ•°æ® (è®­ç»ƒè¾“å…¥)
```

### 8.2 æ•°æ®æ ¼å¼è¯´æ˜

`processed_seqs.csv` çš„æ ¼å¼ï¼š
```csv
user_id,movie_id,rating,unix_timestamp
1,"[924, 919, 2683, ...]","[6, 6, 6, ...]","[1094785598, 1094785621, ...]"
2,"[62, 469, 1121, ...]","[9, 5, 5, ...]","[974820598, 974820598, ...]"
```

æ¯è¡ŒåŒ…å«ï¼š
- `user_id`: ç”¨æˆ·ID
- `movie_id`: ç”µå½±IDåºåˆ— (Pythonåˆ—è¡¨å­—ç¬¦ä¸²)
- `rating`: åŠ¨ä½œåºåˆ— (Pythonåˆ—è¡¨å­—ç¬¦ä¸²)
- `unix_timestamp`: æ—¶é—´æˆ³åºåˆ— (Pythonåˆ—è¡¨å­—ç¬¦ä¸²)

---

## 9. è¿è¡Œè®­ç»ƒ

### 9.1 é…ç½®è®­ç»ƒå‚æ•°

```bash
# æŸ¥çœ‹giné…ç½®æ–‡ä»¶
cat movielen_ranking.gin
```

å…³é”®é…ç½®ï¼š
```python
NetworkArgs.kernel_backend = "cutlass"  # ä½¿ç”¨CUTLASSåç«¯
TensorModelParallelArgs.tensor_model_parallel_size = 1  # A100 FUSEDå±‚åªæ”¯æŒTP=1
```

### 9.2 å¯åŠ¨è®­ç»ƒ (Rankingä»»åŠ¡)

```bash
# ç¡®ä¿åœ¨examples/hstuç›®å½•
cd /path/to/recsys-examples-main/examples/hstu

# å•GPUè®­ç»ƒ
PYTHONPATH=${PYTHONPATH}:$(realpath ../) \
torchrun --nproc_per_node 1 \
         --master_addr localhost \
         --master_port 6000 \
         pretrain_gr_ranking.py \
         --gin-config-file movielen_ranking.gin
```

### 9.3 å¯åŠ¨è®­ç»ƒ (Retrievalä»»åŠ¡)

```bash
# å•GPUè®­ç»ƒ
PYTHONPATH=${PYTHONPATH}:$(realpath ../) \
torchrun --nproc_per_node 1 \
         --master_addr localhost \
         --master_port 6000 \
         pretrain_gr_retrieval.py \
         --gin-config-file movielen_retrieval.gin
```

### 9.4 å¤šGPUè®­ç»ƒ (å¦‚æœæœ‰å¤šå¼ A100)

```bash
# 4 GPUè®­ç»ƒ
PYTHONPATH=${PYTHONPATH}:$(realpath ../) \
torchrun --nproc_per_node 4 \
         --master_addr localhost \
         --master_port 6000 \
         pretrain_gr_ranking.py \
         --gin-config-file movielen_ranking.gin
```

**æœŸæœ›è¾“å‡º**ï¼š
```
[INFO] A100 GPUæ£€æµ‹é€šè¿‡
[INFO] ä½¿ç”¨CUTLASS backend
[INFO] åŠ è½½æ•°æ®: tmp_data/ml-20m/processed_seqs.csv
[INFO] ç”¨æˆ·æ•°: 138493
[INFO] ç‰©å“æ•°: 26744
[INFO] å¼€å§‹è®­ç»ƒ...
Epoch 1/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [05:23<00:00, 3.09it/s, loss=2.345]
```

---

## 10. å¸¸è§é—®é¢˜æ’æŸ¥

### 10.1 CUDAå†…å­˜ä¸è¶³

**é—®é¢˜**ï¼š`RuntimeError: CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# 1. å‡å°‘batch size (åœ¨giné…ç½®æ–‡ä»¶ä¸­)
DataArgs.train_batch_size = 32  # æ”¹ä¸º16æˆ–8

# 2. å¯ç”¨æ¢¯åº¦ç´¯ç§¯
TrainingArgs.gradient_accumulation_steps = 4

# 3. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
TrainingArgs.bf16 = True

# 4. æ¸…ç†CUDAç¼“å­˜
python -c "import torch; torch.cuda.empty_cache()"
```

### 10.2 ç¼–è¯‘é”™è¯¯ï¼šæ‰¾ä¸åˆ°CUTLASS

**é—®é¢˜**ï¼š`fatal error: cutlass/cutlass.h: No such file or directory`

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# æ£€æŸ¥CUTLASSå­æ¨¡å—
cd /path/to/recsys-examples-main
git submodule status third_party/cutlass

# å¦‚æœæœªåˆå§‹åŒ–ï¼Œé‡æ–°åˆå§‹åŒ–
git submodule update --init --recursive third_party/cutlass

# éªŒè¯å¤´æ–‡ä»¶
ls third_party/cutlass/include/cutlass/cutlass.h
```

### 10.3 ç¼–è¯‘é”™è¯¯ï¼šæ‰¾ä¸åˆ°HierarchicalKV

**é—®é¢˜**ï¼šç¼–è¯‘dynamicembæ—¶æ‰¾ä¸åˆ°HierarchicalKVå¤´æ–‡ä»¶

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# åˆå§‹åŒ–HierarchicalKVå­æ¨¡å—
cd /path/to/recsys-examples-main
git submodule update --init --recursive third_party/HierarchicalKV

# éªŒè¯å¤´æ–‡ä»¶
ls third_party/HierarchicalKV/include/
```

### 10.4 è¿è¡Œæ—¶é”™è¯¯ï¼šä¸æ˜¯A100 GPU

**é—®é¢˜**ï¼š`RuntimeError: This code only supports NVIDIA A100 GPU (SM 8.0)`

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# æ£€æŸ¥GPUè®¡ç®—èƒ½åŠ›
python -c "import torch; print(torch.cuda.get_device_capability(0))"

# å¦‚æœä¸æ˜¯(8, 0)ï¼Œè¯´æ˜ä¸æ˜¯A100 GPU
# è§£å†³æ–¹æ¡ˆ1: ä½¿ç”¨A100 GPU
# è§£å†³æ–¹æ¡ˆ2: å¦‚æœæ˜¯å¼€å‘æµ‹è¯•ï¼Œå¯ä»¥ä¸´æ—¶æ³¨é‡Šæ‰check_a100_gpu()è°ƒç”¨ (ä¸æ¨è)
```

### 10.5 TorchRecå¯¼å…¥é”™è¯¯

**é—®é¢˜**ï¼š`ImportError: cannot import name 'xxx' from 'torchrec'`

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# æ£€æŸ¥TorchRecç‰ˆæœ¬
python -c "import torchrec; print(torchrec.__version__)"

# é‡æ–°å®‰è£…æŒ‡å®šç‰ˆæœ¬
cd ~/torchrec
git checkout 6aaf1fa72e884642f39c49ef232162fa3772055e
pip uninstall torchrec -y
pip install --no-deps .
```

### 10.6 Megatron-Coreç‰ˆæœ¬ä¸å…¼å®¹

**é—®é¢˜**ï¼š`AttributeError: module 'megatron.core' has no attribute 'xxx'`

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# ä½¿ç”¨æŒ‡å®šç‰ˆæœ¬
cd ~/megatron-lm
git checkout core_r0.9.0
pip uninstall megatron-core -y
pip install -e .
```

### 10.7 ç¼–è¯‘æ—¶å†…å­˜ä¸è¶³

**é—®é¢˜**ï¼š`g++: internal compiler error: Killed (program cc1plus)`

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# å‡å°‘å¹¶è¡Œç¼–è¯‘ä»»åŠ¡æ•°
export MAX_JOBS=2
export NVCC_THREADS=2

# å…³é—­å…¶ä»–å å†…å­˜çš„ç¨‹åº
# é‡æ–°ç¼–è¯‘
cd corelib/hstu
pip install . --force-reinstall --no-cache-dir
```

### 10.8 NVCCç‰ˆæœ¬ä¸åŒ¹é…

**é—®é¢˜**ï¼š`The detected CUDA version mismatches the version that was used to compile PyTorch`

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# æ£€æŸ¥PyTorchçš„CUDAç‰ˆæœ¬
python -c "import torch; print(torch.version.cuda)"

# æ£€æŸ¥ç³»ç»ŸCUDAç‰ˆæœ¬
nvcc --version

# é‡æ–°å®‰è£…åŒ¹é…ç‰ˆæœ¬çš„PyTorch
# ä¾‹å¦‚ï¼Œå¦‚æœnvccæ˜¯12.1ï¼Œå®‰è£…PyTorch with CUDA 12.1
pip uninstall torch -y
pip install torch --index-url https://download.pytorch.org/whl/cu121
```

---

## 11. æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 11.1 ç¼–è¯‘ä¼˜åŒ–

```bash
# ä½¿ç”¨O3ä¼˜åŒ–çº§åˆ« (é»˜è®¤å·²å¯ç”¨)
export CXXFLAGS="-O3"
export NVCCFLAGS="-O3"

# å¯ç”¨å¿«é€Ÿæ•°å­¦è¿ç®— (é»˜è®¤å·²å¯ç”¨)
# --use_fast_math
```

### 11.2 è¿è¡Œæ—¶ä¼˜åŒ–

```bash
# è®¾ç½®CUDAè®¾å¤‡
export CUDA_VISIBLE_DEVICES=0

# å¯ç”¨TF32 (A100æ”¯æŒï¼ŒåŠ é€Ÿè®­ç»ƒ)
export NVIDIA_TF32_OVERRIDE=1

# è®¾ç½®CUDNNè‡ªåŠ¨è°ƒä¼˜
export CUDNN_BENCHMARK=1

# ç¦ç”¨CUDNNç¡®å®šæ€§ (åŠ é€Ÿä½†ç‰ºç‰²å¯å¤ç°æ€§)
export CUDNN_DETERMINISTIC=0
```

### 11.3 æ•°æ®åŠ è½½ä¼˜åŒ–

```python
# åœ¨giné…ç½®ä¸­å¢åŠ num_workers
DataArgs.num_workers = 8  # æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´
DataArgs.prefetch_factor = 4  # é¢„å–æ‰¹æ¬¡æ•°
```

---

## 12. å®Œæ•´å®‰è£…è„šæœ¬

ä¸ºäº†æ–¹ä¾¿ï¼Œè¿™é‡Œæä¾›ä¸€ä¸ªå®Œæ•´çš„è‡ªåŠ¨åŒ–å®‰è£…è„šæœ¬ï¼š

```bash
#!/bin/bash
# A100ç¯å¢ƒè‡ªåŠ¨é…ç½®è„šæœ¬
# ä½¿ç”¨æ–¹æ³•: bash install_a100_env.sh

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

echo "======================================"
echo "A100 HSTUè®­ç»ƒç¯å¢ƒè‡ªåŠ¨é…ç½®è„šæœ¬"
echo "======================================"

# 1. æ£€æŸ¥GPU
echo "[1/12] æ£€æŸ¥GPU..."
nvidia-smi --query-gpu=gpu_name,compute_cap --format=csv
COMPUTE_CAP=$(nvidia-smi --query-gpu=compute_cap --format=csv,noheader | head -n1)
if [ "$COMPUTE_CAP" != "8.0" ]; then
    echo "è­¦å‘Š: å½“å‰GPUè®¡ç®—èƒ½åŠ›ä¸º $COMPUTE_CAPï¼Œä¸æ˜¯A100 (8.0)"
    read -p "æ˜¯å¦ç»§ç»­? (y/n) " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        exit 1
    fi
fi

# 2. æ£€æŸ¥CUDA
echo "[2/12] æ£€æŸ¥CUDA..."
nvcc --version

# 3. åˆ›å»ºcondaç¯å¢ƒ
echo "[3/12] åˆ›å»ºcondaç¯å¢ƒ..."
conda create -n hstu_a100 python=3.10 -y
source $(conda info --base)/etc/profile.d/conda.sh
conda activate hstu_a100

# 4. å®‰è£…PyTorch
echo "[4/12] å®‰è£…PyTorch..."
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# 5. å®‰è£…åŸºç¡€ä¾èµ–
echo "[5/12] å®‰è£…åŸºç¡€ä¾èµ–..."
pip install --upgrade pip setuptools wheel
pip install ninja psutil packaging einops

# 6. ç¼–è¯‘FBGEMM_GPU
echo "[6/12] ç¼–è¯‘FBGEMM_GPU (éœ€è¦10-30åˆ†é’Ÿ)..."
cd ~
pip install --no-cache setuptools==69.5.1 setuptools-git-versioning scikit-build
git clone --recursive -b main https://github.com/pytorch/FBGEMM.git fbgemm
cd fbgemm/fbgemm_gpu
git checkout 642ccb980d05aa1be00ccd131c5991b0914e2e64
python setup.py install --package_variant=cuda -DTORCH_CUDA_ARCH_LIST="8.0"

# 7. å®‰è£…TorchRec
echo "[7/12] å®‰è£…TorchRec..."
cd ~
pip install --no-deps tensordict orjson
git clone --recursive -b main https://github.com/pytorch/torchrec.git torchrec
cd torchrec
git checkout 6aaf1fa72e884642f39c49ef232162fa3772055e
pip install --no-deps .

# 8. å®‰è£…Megatron-Core
echo "[8/12] å®‰è£…Megatron-Core..."
cd ~
git clone -b core_r0.9.0 https://github.com/NVIDIA/Megatron-LM.git megatron-lm
cd megatron-lm
pip install -e .

# 9. å®‰è£…å…¶ä»–ä¾èµ–
echo "[9/12] å®‰è£…å…¶ä»–Pythonä¾èµ–..."
pip install torchx gin-config torchmetrics==1.0.3 typing-extensions iopath

# 10. åˆå§‹åŒ–å­æ¨¡å—
echo "[10/12] åˆå§‹åŒ–Gitå­æ¨¡å—..."
cd /path/to/recsys-examples-main  # ä¿®æ”¹ä¸ºå®é™…è·¯å¾„
git submodule update --init third_party/cutlass
git submodule update --init third_party/HierarchicalKV

# 11. ç¼–è¯‘CUDAåŠ é€Ÿæ¨¡å—
echo "[11/12] ç¼–è¯‘CUDAåŠ é€Ÿæ¨¡å— (éœ€è¦30-60åˆ†é’Ÿ)..."
cd corelib/hstu
export HSTU_DISABLE_86OR89=TRUE
export HSTU_DISABLE_ARBITRARY=TRUE
export HSTU_DISABLE_LOCAL=TRUE
export HSTU_DISABLE_RAB=TRUE
export HSTU_DISABLE_DRAB=TRUE
pip install .

cd ../dynamicemb
python setup.py install

cd ../../examples/hstu
python setup.py install

# 12. éªŒè¯å®‰è£…
echo "[12/12] éªŒè¯å®‰è£…..."
python -c '
import torch
import fbgemm_gpu
import torchrec
import hstu_attn
import dynamicemb
import hstu_cuda_ops
print("\nğŸ‰ æ‰€æœ‰æ¨¡å—å®‰è£…æˆåŠŸï¼")
print(f"PyTorch: {torch.__version__}")
print(f"CUDA: {torch.version.cuda}")
print(f"GPU: {torch.cuda.get_device_name(0)}")
print(f"Compute Cap: {torch.cuda.get_device_capability(0)}")
'

echo "======================================"
echo "âœ“ ç¯å¢ƒé…ç½®å®Œæˆï¼"
echo "======================================"
echo ""
echo "ä¸‹ä¸€æ­¥ï¼š"
echo "1. å‡†å¤‡æ•°æ®: python preprocessor.py --dataset_name ml-20m"
echo "2. å¼€å§‹è®­ç»ƒ: torchrun --nproc_per_node 1 pretrain_gr_ranking.py --gin-config-file movielen_ranking.gin"
echo ""
```

---

## 13. æ€»ç»“

### å…³é”®æ£€æŸ¥æ¸…å•

- [ ] GPUå‹å·ä¸ºA100ï¼Œè®¡ç®—èƒ½åŠ›ä¸º8.0
- [ ] CUDAç‰ˆæœ¬ >= 11.6 (æ¨è12.1+)
- [ ] PyTorchå®‰è£…æˆåŠŸï¼ŒCUDAå¯ç”¨
- [ ] FBGEMM_GPUç¼–è¯‘æˆåŠŸ
- [ ] TorchRecå®‰è£…æˆåŠŸ
- [ ] Megatron-Coreå®‰è£…æˆåŠŸ
- [ ] CUTLASSå­æ¨¡å—åˆå§‹åŒ–
- [ ] HierarchicalKVå­æ¨¡å—åˆå§‹åŒ–
- [ ] HSTU Attention (CUTLASS)ç¼–è¯‘æˆåŠŸ
- [ ] Dynamic Embeddingsç¼–è¯‘æˆåŠŸ
- [ ] HSTUè®­ç»ƒç®—å­ç¼–è¯‘æˆåŠŸ
- [ ] æ•°æ®å¤„ç†å®Œæˆ
- [ ] è®­ç»ƒè„šæœ¬å¯ä»¥è¿è¡Œ

### é¢„è®¡æ—¶é—´

| æ­¥éª¤ | æ—¶é—´ |
|------|------|
| ç³»ç»Ÿæ£€æŸ¥ | 5åˆ†é’Ÿ |
| CUDAå®‰è£… | 10åˆ†é’Ÿ |
| åˆ›å»ºcondaç¯å¢ƒ | 5åˆ†é’Ÿ |
| å®‰è£…PyTorch | 10åˆ†é’Ÿ |
| ç¼–è¯‘FBGEMM_GPU | 10-30åˆ†é’Ÿ |
| å®‰è£…TorchRec | 5åˆ†é’Ÿ |
| å®‰è£…Megatron-Core | 5åˆ†é’Ÿ |
| ç¼–è¯‘HSTU Attention | 30-60åˆ†é’Ÿ |
| ç¼–è¯‘DynamicEmb | 5-15åˆ†é’Ÿ |
| ç¼–è¯‘è®­ç»ƒç®—å­ | 2-5åˆ†é’Ÿ |
| æ•°æ®å‡†å¤‡ | 10åˆ†é’Ÿ |
| **æ€»è®¡** | **1.5-2.5å°æ—¶** |

### ç¡¬ä»¶è¦æ±‚

| èµ„æº | æœ€ä½é…ç½® | æ¨èé…ç½® |
|------|----------|----------|
| GPU | NVIDIA A100 40GB | NVIDIA A100 80GB |
| CPU | 16æ ¸ | 32æ ¸+ |
| RAM | 64GB | 128GB+ |
| ç£ç›˜ | 100GB | 500GB+ |
| æ“ä½œç³»ç»Ÿ | Ubuntu 20.04 | Ubuntu 22.04 |

---

**ç¥æ‚¨é…ç½®é¡ºåˆ©ï¼å¦‚æœ‰é—®é¢˜ï¼Œè¯·å‚è€ƒ"å¸¸è§é—®é¢˜æ’æŸ¥"ç« èŠ‚ã€‚** ğŸš€

