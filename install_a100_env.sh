#!/bin/bash
# ============================================
# A100 HSTUè®­ç»ƒç¯å¢ƒè‡ªåŠ¨é…ç½®è„šæœ¬
# ä½œè€…: AI Assistant
# ä½¿ç”¨æ–¹æ³•: bash install_a100_env.sh
# ============================================

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# æ‰“å°å‡½æ•°
print_header() {
    echo -e "${BLUE}=====================================${NC}"
    echo -e "${BLUE}$1${NC}"
    echo -e "${BLUE}=====================================${NC}"
}

print_step() {
    echo -e "${GREEN}[$1] $2${NC}"
}

print_warning() {
    echo -e "${YELLOW}è­¦å‘Š: $1${NC}"
}

print_error() {
    echo -e "${RED}é”™è¯¯: $1${NC}"
}

# æ£€æŸ¥å‘½ä»¤æ˜¯å¦å­˜åœ¨
command_exists() {
    command -v "$1" >/dev/null 2>&1
}

# ============================================
# å¼€å§‹å®‰è£…
# ============================================

print_header "A100 HSTUè®­ç»ƒç¯å¢ƒè‡ªåŠ¨é…ç½®è„šæœ¬"

# ============================================
# 1. æ£€æŸ¥GPU
# ============================================
print_step "1/12" "æ£€æŸ¥GPU..."

if ! command_exists nvidia-smi; then
    print_error "nvidia-smiæœªæ‰¾åˆ°ï¼Œè¯·å…ˆå®‰è£…NVIDIAé©±åŠ¨"
    exit 1
fi

nvidia-smi --query-gpu=gpu_name,compute_cap --format=csv

COMPUTE_CAP=$(nvidia-smi --query-gpu=compute_cap --format=csv,noheader | head -n1 | tr -d ' ')
echo "æ£€æµ‹åˆ°è®¡ç®—èƒ½åŠ›: $COMPUTE_CAP"

if [ "$COMPUTE_CAP" != "8.0" ]; then
    print_warning "å½“å‰GPUè®¡ç®—èƒ½åŠ›ä¸º $COMPUTE_CAPï¼Œä¸æ˜¯A100 (8.0)"
    read -p "æ˜¯å¦ç»§ç»­? (y/n) " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        exit 1
    fi
fi

# ============================================
# 2. æ£€æŸ¥CUDA
# ============================================
print_step "2/12" "æ£€æŸ¥CUDA..."

if ! command_exists nvcc; then
    print_error "nvccæœªæ‰¾åˆ°ï¼Œè¯·å…ˆå®‰è£…CUDA Toolkit"
    exit 1
fi

nvcc --version

# ============================================
# 3. æ£€æŸ¥conda
# ============================================
print_step "3/12" "æ£€æŸ¥conda..."

if ! command_exists conda; then
    print_error "condaæœªæ‰¾åˆ°ï¼Œè¯·å…ˆå®‰è£…Minicondaæˆ–Anaconda"
    exit 1
fi

# ============================================
# 4. åˆ›å»ºcondaç¯å¢ƒ
# ============================================
print_step "4/12" "åˆ›å»ºcondaç¯å¢ƒ hstu_a100..."

# æ£€æŸ¥ç¯å¢ƒæ˜¯å¦å·²å­˜åœ¨
if conda env list | grep -q "hstu_a100"; then
    print_warning "ç¯å¢ƒ hstu_a100 å·²å­˜åœ¨"
    read -p "æ˜¯å¦åˆ é™¤å¹¶é‡æ–°åˆ›å»º? (y/n) " -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        conda env remove -n hstu_a100 -y
        conda create -n hstu_a100 python=3.10 -y
    fi
else
    conda create -n hstu_a100 python=3.10 -y
fi

# æ¿€æ´»ç¯å¢ƒ
source $(conda info --base)/etc/profile.d/conda.sh
conda activate hstu_a100

# ============================================
# 5. å®‰è£…PyTorch
# ============================================
print_step "5/12" "å®‰è£…PyTorch..."

pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# éªŒè¯PyTorch
python -c "
import torch
print(f'PyTorch: {torch.__version__}')
print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')
print(f'GPU: {torch.cuda.get_device_name(0)}')
assert torch.cuda.is_available(), 'CUDAä¸å¯ç”¨'
"

# ============================================
# 6. å®‰è£…åŸºç¡€ä¾èµ–
# ============================================
print_step "6/12" "å®‰è£…åŸºç¡€ä¾èµ–..."

pip install --upgrade pip setuptools wheel
pip install ninja psutil packaging einops

# ============================================
# 7. ç¼–è¯‘FBGEMM_GPU
# ============================================
print_step "7/12" "ç¼–è¯‘FBGEMM_GPU (éœ€è¦10-30åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…)..."

cd ~
pip install --no-cache setuptools==69.5.1 setuptools-git-versioning scikit-build

if [ -d "fbgemm" ]; then
    print_warning "fbgemmç›®å½•å·²å­˜åœ¨ï¼Œè·³è¿‡å…‹éš†"
else
    git clone --recursive -b main https://github.com/pytorch/FBGEMM.git fbgemm
fi

cd fbgemm/fbgemm_gpu
git checkout 642ccb980d05aa1be00ccd131c5991b0914e2e64

# ç¼–è¯‘ (åªä¸ºA100çš„SM 8.0ç¼–è¯‘)
MAX_JOBS=4 python setup.py install --package_variant=cuda -DTORCH_CUDA_ARCH_LIST="8.0"

# éªŒè¯
python -c "import fbgemm_gpu; print(f'FBGEMM_GPU: {fbgemm_gpu.__version__}')"

# ============================================
# 8. å®‰è£…TorchRec
# ============================================
print_step "8/12" "å®‰è£…TorchRec..."

cd ~
pip install --no-deps tensordict orjson

if [ -d "torchrec" ]; then
    print_warning "torchrecç›®å½•å·²å­˜åœ¨ï¼Œè·³è¿‡å…‹éš†"
else
    git clone --recursive -b main https://github.com/pytorch/torchrec.git torchrec
fi

cd torchrec
git checkout 6aaf1fa72e884642f39c49ef232162fa3772055e
pip install --no-deps .

# éªŒè¯
python -c "import torchrec; print(f'TorchRec: {torchrec.__version__}')"

# ============================================
# 9. å®‰è£…Megatron-Core
# ============================================
print_step "9/12" "å®‰è£…Megatron-Core..."

cd ~

if [ -d "megatron-lm" ]; then
    print_warning "megatron-lmç›®å½•å·²å­˜åœ¨ï¼Œè·³è¿‡å…‹éš†"
else
    git clone -b core_r0.9.0 https://github.com/NVIDIA/Megatron-LM.git megatron-lm
fi

cd megatron-lm
pip install -e .

# éªŒè¯
python -c "import megatron; print('Megatron-Coreå®‰è£…æˆåŠŸ')"

# ============================================
# 10. å®‰è£…å…¶ä»–ä¾èµ–
# ============================================
print_step "10/12" "å®‰è£…å…¶ä»–Pythonä¾èµ–..."

pip install torchx gin-config torchmetrics==1.0.3 typing-extensions iopath

# ============================================
# 11. è¯¢é—®é¡¹ç›®è·¯å¾„
# ============================================
print_header "é¡¹ç›®è·¯å¾„é…ç½®"

echo "è¯·è¾“å…¥recsys-examples-mainçš„å®Œæ•´è·¯å¾„:"
echo "ä¾‹å¦‚: /home/user/recsys-examples-main"
read -p "è·¯å¾„: " PROJECT_PATH

# éªŒè¯è·¯å¾„
if [ ! -d "$PROJECT_PATH" ]; then
    print_error "è·¯å¾„ä¸å­˜åœ¨: $PROJECT_PATH"
    exit 1
fi

if [ ! -f "$PROJECT_PATH/README.md" ]; then
    print_error "è·¯å¾„ä¸‹æ²¡æœ‰æ‰¾åˆ°README.mdï¼Œè¯·ç¡®è®¤æ˜¯å¦ä¸ºæ­£ç¡®çš„é¡¹ç›®è·¯å¾„"
    exit 1
fi

cd "$PROJECT_PATH"

# ============================================
# 12. åˆå§‹åŒ–å­æ¨¡å—
# ============================================
print_step "11/12" "åˆå§‹åŒ–Gitå­æ¨¡å—..."

git submodule update --init third_party/cutlass
git submodule update --init third_party/HierarchicalKV

# éªŒè¯å­æ¨¡å—
if [ ! -f "third_party/cutlass/include/cutlass/cutlass.h" ]; then
    print_error "CUTLASSå­æ¨¡å—åˆå§‹åŒ–å¤±è´¥"
    exit 1
fi

if [ ! -d "third_party/HierarchicalKV/include" ]; then
    print_error "HierarchicalKVå­æ¨¡å—åˆå§‹åŒ–å¤±è´¥"
    exit 1
fi

# ============================================
# 13. ç¼–è¯‘CUDAåŠ é€Ÿæ¨¡å—
# ============================================
print_step "12/12" "ç¼–è¯‘CUDAåŠ é€Ÿæ¨¡å— (éœ€è¦30-60åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…)..."

# 13.1 ç¼–è¯‘HSTU Attention
echo "  â†’ ç¼–è¯‘HSTU Attention (CUTLASSå†…æ ¸)..."
cd "$PROJECT_PATH/corelib/hstu"

export HSTU_DISABLE_86OR89=TRUE
export HSTU_DISABLE_ARBITRARY=TRUE
export HSTU_DISABLE_LOCAL=TRUE
export HSTU_DISABLE_RAB=TRUE
export HSTU_DISABLE_DRAB=TRUE
export NVCC_THREADS=4
export MAX_JOBS=4

pip install .

# éªŒè¯
python -c "import hstu_attn; print('HSTU Attentionå®‰è£…æˆåŠŸ')"

# 13.2 ç¼–è¯‘Dynamic Embeddings
echo "  â†’ ç¼–è¯‘Dynamic Embeddings..."
cd "$PROJECT_PATH/corelib/dynamicemb"

python setup.py install

# éªŒè¯
python -c "import dynamicemb; print(f'DynamicEmb: {dynamicemb.__version__}')"

# 13.3 ç¼–è¯‘HSTUè®­ç»ƒç®—å­
echo "  â†’ ç¼–è¯‘HSTUè®­ç»ƒç®—å­..."
cd "$PROJECT_PATH/examples/hstu"

python setup.py install

# éªŒè¯
python -c "import hstu_cuda_ops; import paged_kvcache_ops; print('HSTUè®­ç»ƒç®—å­å®‰è£…æˆåŠŸ')"

# ============================================
# 14. å®Œæ•´éªŒè¯
# ============================================
print_header "éªŒè¯å®‰è£…"

python -c '
import torch
print(f"âœ“ PyTorch {torch.__version__}")
print(f"âœ“ CUDA {torch.version.cuda}")
print(f"âœ“ GPU: {torch.cuda.get_device_name(0)}")
print(f"âœ“ Compute Capability: {torch.cuda.get_device_capability(0)}")

import fbgemm_gpu
print(f"âœ“ FBGEMM_GPU {fbgemm_gpu.__version__}")

import torchrec
print(f"âœ“ TorchRec {torchrec.__version__}")

import megatron
print(f"âœ“ Megatron-Core")

import hstu_attn
print(f"âœ“ HSTU Attention (CUTLASS)")

import dynamicemb
print(f"âœ“ DynamicEmb {dynamicemb.__version__}")

import hstu_cuda_ops
import paged_kvcache_ops
print(f"âœ“ HSTU CUDA Ops")

print("\nğŸ‰ æ‰€æœ‰æ¨¡å—å®‰è£…æˆåŠŸï¼")
'

# ============================================
# å®Œæˆ
# ============================================
print_header "å®‰è£…å®Œæˆï¼"

echo ""
echo "ç¯å¢ƒå·²æˆåŠŸé…ç½®åˆ°condaç¯å¢ƒ: hstu_a100"
echo "é¡¹ç›®è·¯å¾„: $PROJECT_PATH"
echo ""
echo -e "${GREEN}ä¸‹ä¸€æ­¥æ“ä½œ:${NC}"
echo ""
echo "1. æ¿€æ´»ç¯å¢ƒ:"
echo "   conda activate hstu_a100"
echo ""
echo "2. è¿›å…¥é¡¹ç›®ç›®å½•:"
echo "   cd $PROJECT_PATH/examples/hstu"
echo ""
echo "3. å‡†å¤‡æ•°æ®:"
echo "   mkdir -p ./tmp_data"
echo "   python preprocessor.py --dataset_name ml-20m"
echo ""
echo "4. å¼€å§‹è®­ç»ƒ (Rankingä»»åŠ¡):"
echo "   PYTHONPATH=\${PYTHONPATH}:\$(realpath ../) \\"
echo "   torchrun --nproc_per_node 1 \\"
echo "            --master_addr localhost \\"
echo "            --master_port 6000 \\"
echo "            pretrain_gr_ranking.py \\"
echo "            --gin-config-file movielen_ranking.gin"
echo ""
echo "5. å¼€å§‹è®­ç»ƒ (Retrievalä»»åŠ¡):"
echo "   PYTHONPATH=\${PYTHONPATH}:\$(realpath ../) \\"
echo "   torchrun --nproc_per_node 1 \\"
echo "            --master_addr localhost \\"
echo "            --master_port 6000 \\"
echo "            pretrain_gr_retrieval.py \\"
echo "            --gin-config-file movielen_retrieval.gin"
echo ""
echo -e "${BLUE}æ›´å¤šä¿¡æ¯è¯·æŸ¥çœ‹: A100ç¯å¢ƒé…ç½®å®Œæ•´æ•™ç¨‹.md${NC}"
echo ""

