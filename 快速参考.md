# ğŸš€ A100ç¯å¢ƒé…ç½®å¿«é€Ÿå‚è€ƒ

## âš¡ ä¸€é”®å®‰è£…

```bash
# ä¸‹è½½å¹¶è¿è¡Œè‡ªåŠ¨åŒ–è„šæœ¬
bash install_a100_env.sh
```

---

## ğŸ“ å…³é”®æ£€æŸ¥å‘½ä»¤

### GPUæ£€æŸ¥
```bash
# GPUå‹å·å’Œè®¡ç®—èƒ½åŠ›
nvidia-smi --query-gpu=gpu_name,compute_cap --format=csv
# æœŸæœ›: NVIDIA A100, 8.0

# CUDAé©±åŠ¨ç‰ˆæœ¬
nvidia-smi | grep "CUDA Version"
# æœŸæœ›: 12.x æˆ– 11.6+
```

### CUDAå·¥å…·é“¾æ£€æŸ¥
```bash
# NVCCç‰ˆæœ¬
nvcc --version
# æœŸæœ›: CUDA 12.1+ æˆ– 11.6+

# CUDAç¯å¢ƒå˜é‡
echo $CUDA_HOME
echo $PATH | grep cuda
echo $LD_LIBRARY_PATH | grep cuda
```

### Pythonç¯å¢ƒæ£€æŸ¥
```bash
# æ¿€æ´»ç¯å¢ƒ
conda activate hstu_a100

# PyTorchå’ŒCUDA
python -c "
import torch
print(f'PyTorch: {torch.__version__}')
print(f'CUDA: {torch.version.cuda}')
print(f'GPU: {torch.cuda.get_device_name(0)}')
print(f'Compute Cap: {torch.cuda.get_device_capability(0)}')  # å¿…é¡»æ˜¯ (8, 0)
"

# å…³é”®ä¾èµ–
python -c "
import fbgemm_gpu
import torchrec
import megatron
import hstu_attn
import dynamicemb
import hstu_cuda_ops
print('âœ“ æ‰€æœ‰ä¾èµ–å·²å®‰è£…')
"
```

---

## ğŸ”§ æ‰‹åŠ¨å®‰è£…æ­¥éª¤

### 1. åˆ›å»ºç¯å¢ƒ
```bash
conda create -n hstu_a100 python=3.10 -y
conda activate hstu_a100
```

### 2. å®‰è£…PyTorch
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### 3. ç¼–è¯‘FBGEMM_GPU
```bash
pip install --no-cache setuptools==69.5.1 setuptools-git-versioning scikit-build
git clone --recursive -b main https://github.com/pytorch/FBGEMM.git ~/fbgemm
cd ~/fbgemm/fbgemm_gpu
git checkout 642ccb980d05aa1be00ccd131c5991b0914e2e64
python setup.py install --package_variant=cuda -DTORCH_CUDA_ARCH_LIST="8.0"
```

### 4. å®‰è£…TorchRec
```bash
pip install --no-deps tensordict orjson
git clone --recursive -b main https://github.com/pytorch/torchrec.git ~/torchrec
cd ~/torchrec
git checkout 6aaf1fa72e884642f39c49ef232162fa3772055e
pip install --no-deps .
```

### 5. å®‰è£…Megatron-Core
```bash
git clone -b core_r0.9.0 https://github.com/NVIDIA/Megatron-LM.git ~/megatron-lm
cd ~/megatron-lm
pip install -e .
```

### 6. å®‰è£…å…¶ä»–ä¾èµ–
```bash
pip install torchx gin-config torchmetrics==1.0.3 typing-extensions iopath ninja psutil packaging einops
```

### 7. åˆå§‹åŒ–å­æ¨¡å—
```bash
cd /path/to/recsys-examples-main
git submodule update --init third_party/cutlass
git submodule update --init third_party/HierarchicalKV
```

### 8. ç¼–è¯‘CUDAæ¨¡å—
```bash
# HSTU Attention
cd corelib/hstu
export HSTU_DISABLE_86OR89=TRUE
export HSTU_DISABLE_ARBITRARY=TRUE
export HSTU_DISABLE_LOCAL=TRUE
export HSTU_DISABLE_RAB=TRUE
export HSTU_DISABLE_DRAB=TRUE
pip install .

# Dynamic Embeddings
cd ../dynamicemb
python setup.py install

# HSTUè®­ç»ƒç®—å­
cd ../../examples/hstu
python setup.py install
```

---

## ğŸƒ è¿è¡Œè®­ç»ƒ

### æ•°æ®å‡†å¤‡
```bash
cd /path/to/recsys-examples-main/examples/hstu
mkdir -p ./tmp_data
python preprocessor.py --dataset_name ml-20m
```

### Rankingä»»åŠ¡
```bash
PYTHONPATH=${PYTHONPATH}:$(realpath ../) \
torchrun --nproc_per_node 1 \
         --master_addr localhost \
         --master_port 6000 \
         pretrain_gr_ranking.py \
         --gin-config-file movielen_ranking.gin
```

### Retrievalä»»åŠ¡
```bash
PYTHONPATH=${PYTHONPATH}:$(realpath ../) \
torchrun --nproc_per_node 1 \
         --master_addr localhost \
         --master_port 6000 \
         pretrain_gr_retrieval.py \
         --gin-config-file movielen_retrieval.gin
```

### å¤šGPUè®­ç»ƒ
```bash
# 4 GPUè®­ç»ƒ
PYTHONPATH=${PYTHONPATH}:$(realpath ../) \
torchrun --nproc_per_node 4 \
         --master_addr localhost \
         --master_port 6000 \
         pretrain_gr_ranking.py \
         --gin-config-file movielen_ranking.gin
```

---

## ğŸ› å¿«é€Ÿé—®é¢˜æ’æŸ¥

### CUDAå†…å­˜ä¸è¶³
```bash
# ç¼–è¾‘giné…ç½®ï¼Œå‡å°‘batch size
DataArgs.train_batch_size = 16  # æˆ– 8

# å¯ç”¨æ··åˆç²¾åº¦
TrainingArgs.bf16 = True

# å¯ç”¨æ¢¯åº¦ç´¯ç§¯
TrainingArgs.gradient_accumulation_steps = 4
```

### ç¼–è¯‘æ—¶å†…å­˜ä¸è¶³
```bash
# å‡å°‘å¹¶è¡Œç¼–è¯‘ä»»åŠ¡
export MAX_JOBS=2
export NVCC_THREADS=2

# é‡æ–°ç¼–è¯‘
cd corelib/hstu
pip install . --force-reinstall --no-cache-dir
```

### CUTLASSæ‰¾ä¸åˆ°
```bash
# é‡æ–°åˆå§‹åŒ–å­æ¨¡å—
cd /path/to/recsys-examples-main
git submodule update --init --recursive third_party/cutlass

# éªŒè¯
ls third_party/cutlass/include/cutlass/cutlass.h
```

### HierarchicalKVæ‰¾ä¸åˆ°
```bash
# é‡æ–°åˆå§‹åŒ–å­æ¨¡å—
git submodule update --init --recursive third_party/HierarchicalKV

# éªŒè¯
ls third_party/HierarchicalKV/include/
```

### PyTorch CUDAç‰ˆæœ¬ä¸åŒ¹é…
```bash
# æ£€æŸ¥ç‰ˆæœ¬
python -c "import torch; print(torch.version.cuda)"
nvcc --version

# é‡æ–°å®‰è£…åŒ¹é…çš„PyTorch
pip uninstall torch -y
pip install torch --index-url https://download.pytorch.org/whl/cu121  # æ ¹æ®CUDAç‰ˆæœ¬è°ƒæ•´
```

---

## ğŸ“Š å…³é”®é…ç½®å‚æ•°

### ç¼–è¯‘æ—¶ç¯å¢ƒå˜é‡
```bash
# A100ä¸“ç”¨é…ç½®
export HSTU_DISABLE_86OR89=TRUE      # ç¦ç”¨SM 8.9 (ä¸æ˜¯A100)
export HSTU_DISABLE_ARBITRARY=TRUE   # ç¦ç”¨ä»»æ„mask
export HSTU_DISABLE_LOCAL=TRUE       # ç¦ç”¨å±€éƒ¨mask
export HSTU_DISABLE_RAB=TRUE         # ç¦ç”¨ç›¸å¯¹ä½ç½®åç½®
export HSTU_DISABLE_DRAB=TRUE        # ç¦ç”¨åŠ¨æ€ç›¸å¯¹ä½ç½®åç½®

# ç¼–è¯‘æ€§èƒ½ä¼˜åŒ–
export NVCC_THREADS=4                # NVCCå¹¶è¡Œçº¿ç¨‹
export MAX_JOBS=4                    # Makeå¹¶è¡Œä»»åŠ¡æ•°

# CUDAæ¶æ„
export TORCH_CUDA_ARCH_LIST="8.0"    # åªä¸ºA100ç¼–è¯‘
```

### è¿è¡Œæ—¶ç¯å¢ƒå˜é‡
```bash
# æ€§èƒ½ä¼˜åŒ–
export CUDA_VISIBLE_DEVICES=0        # æŒ‡å®šGPU
export NVIDIA_TF32_OVERRIDE=1        # å¯ç”¨TF32 (A100åŠ é€Ÿ)
export CUDNN_BENCHMARK=1             # CUDNNè‡ªåŠ¨è°ƒä¼˜
```

### Giné…ç½®å…³é”®å‚æ•°
```python
# ç½‘ç»œé…ç½®
NetworkArgs.kernel_backend = "cutlass"  # ä½¿ç”¨CUTLASSåç«¯ (å¿…é¡»)

# å¼ é‡å¹¶è¡Œ
TensorModelParallelArgs.tensor_model_parallel_size = 1  # A100 FUSEDå±‚åªæ”¯æŒTP=1 (å¿…é¡»)

# æ•°æ®åŠ è½½
DataArgs.train_batch_size = 32       # æ‰¹æ¬¡å¤§å°
DataArgs.num_workers = 8             # æ•°æ®åŠ è½½çº¿ç¨‹

# è®­ç»ƒé…ç½®
TrainingArgs.bf16 = True             # BF16æ··åˆç²¾åº¦ (A100æ¨è)
TrainingArgs.gradient_accumulation_steps = 1  # æ¢¯åº¦ç´¯ç§¯
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### A100ç‰¹æ€§åˆ©ç”¨
```bash
# å¯ç”¨TF32 (è‡ªåŠ¨åŠ é€ŸFP32ï¼ŒA100ç‰¹æ€§)
export NVIDIA_TF32_OVERRIDE=1

# ä½¿ç”¨BF16æ··åˆç²¾åº¦
TrainingArgs.bf16 = True  # åœ¨giné…ç½®ä¸­

# åˆ©ç”¨A100é«˜å¸¦å®½å†…å­˜ (HBM2)
DataArgs.pin_memory = True
```

### ç¼–è¯‘ä¼˜åŒ–
```bash
# ä½¿ç”¨Ninjaæ„å»ºç³»ç»Ÿ (æ›´å¿«)
pip install ninja

# å¹¶è¡Œç¼–è¯‘ (æ ¹æ®CPUæ ¸å¿ƒæ•°å’Œå†…å­˜è°ƒæ•´)
export MAX_JOBS=4
export NVCC_THREADS=4
```

### æ•°æ®åŠ è½½ä¼˜åŒ–
```python
# åœ¨giné…ç½®ä¸­
DataArgs.num_workers = 8              # CPUæ ¸å¿ƒæ•°çš„ä¸€åŠ
DataArgs.prefetch_factor = 4          # é¢„å–æ‰¹æ¬¡æ•°
DataArgs.pin_memory = True            # å›ºå®šå†…å­˜ (åŠ é€ŸGPUä¼ è¾“)
DataArgs.persistent_workers = True    # æŒä¹…åŒ–worker (å‡å°‘å¯åŠ¨å¼€é”€)
```

---

## â±ï¸ é¢„è®¡æ—¶é—´å’Œèµ„æº

### ç¼–è¯‘æ—¶é—´
| æ¨¡å— | æ—¶é—´ |
|------|------|
| FBGEMM_GPU | 10-30åˆ†é’Ÿ |
| HSTU Attention | 30-60åˆ†é’Ÿ |
| DynamicEmb | 5-15åˆ†é’Ÿ |
| è®­ç»ƒç®—å­ | 2-5åˆ†é’Ÿ |
| **æ€»è®¡** | **~1.5-2å°æ—¶** |

### ç¡¬ä»¶è¦æ±‚
| èµ„æº | æœ€ä½ | æ¨è |
|------|------|------|
| GPU | A100 40GB | A100 80GB |
| CPU | 16æ ¸ | 32æ ¸+ |
| RAM | 64GB | 128GB+ |
| ç£ç›˜ | 100GB | 500GB+ |

### ç£ç›˜ç©ºé—´å ç”¨
```
condaç¯å¢ƒ:           ~10GB
PyTorch:            ~3GB
FBGEMM_GPU:         ~2GB
TorchRec:           ~500MB
HSTUç¼–è¯‘äº§ç‰©:        ~5GB
æ•°æ®é›† (ml-20m):     ~1GB
æ€»è®¡:               ~22GB
```

---

## ğŸ“š é‡è¦æ–‡ä»¶ä½ç½®

### é…ç½®æ–‡ä»¶
```
examples/hstu/movielen_ranking.gin      # Rankingä»»åŠ¡é…ç½®
examples/hstu/movielen_retrieval.gin    # Retrievalä»»åŠ¡é…ç½®
examples/hstu/kuairand_1k_ranking.gin   # KuaiRandé…ç½®
```

### æºä»£ç 
```
corelib/hstu/                           # HSTU Attentionæºç 
corelib/dynamicemb/                     # Dynamic Embeddingsæºç 
examples/hstu/modules/                  # HSTUæ¨¡å‹æ¨¡å—
examples/hstu/ops/                      # CUDAç®—å­
examples/hstu/training/                 # è®­ç»ƒé€»è¾‘
```

### ä¾èµ–é¡¹
```
third_party/cutlass/                    # NVIDIA CUTLASSåº“
third_party/HierarchicalKV/             # HierarchicalKVå“ˆå¸Œè¡¨
```

### æ•°æ®ç›®å½•
```
examples/hstu/tmp_data/ml-20m/          # MovieLens-20Mæ•°æ®
examples/hstu/tmp_data/ml-1m/           # MovieLens-1Mæ•°æ®
examples/hstu/tmp_data/kuairand-*/      # KuaiRandæ•°æ®
```

---

## ğŸ”— æœ‰ç”¨çš„é“¾æ¥

- [å®Œæ•´æ•™ç¨‹](./A100ç¯å¢ƒé…ç½®å®Œæ•´æ•™ç¨‹.md)
- [HSTU Attention README](../../corelib/hstu/README.md)
- [Dynamic Embeddings README](../../corelib/dynamicemb/README.md)
- [HSTU Example README](../../examples/hstu/README.md)
- [NVIDIA CUTLASS](https://github.com/NVIDIA/cutlass)
- [HierarchicalKV](https://github.com/NVIDIA-Merlin/HierarchicalKV)
- [PyTorch](https://pytorch.org/)
- [TorchRec](https://github.com/pytorch/torchrec)
- [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)

---

## âœ… å®Œæ•´éªŒè¯è„šæœ¬

```bash
#!/bin/bash
# è¿è¡Œæ­¤è„šæœ¬éªŒè¯æ‰€æœ‰ä¾èµ–æ˜¯å¦æ­£ç¡®å®‰è£…

conda activate hstu_a100

python << EOF
import sys

def check(name, func):
    try:
        func()
        print(f"âœ“ {name}")
        return True
    except Exception as e:
        print(f"âœ— {name}: {e}")
        return False

checks = []

# PyTorch
checks.append(check("PyTorch", lambda: __import__('torch')))

# CUDA
def check_cuda():
    import torch
    assert torch.cuda.is_available(), "CUDAä¸å¯ç”¨"
    assert torch.cuda.get_device_capability(0) == (8, 0), "ä¸æ˜¯A100 GPU"

checks.append(check("CUDA + A100", check_cuda))

# FBGEMM_GPU
checks.append(check("FBGEMM_GPU", lambda: __import__('fbgemm_gpu')))

# TorchRec
checks.append(check("TorchRec", lambda: __import__('torchrec')))

# Megatron-Core
checks.append(check("Megatron-Core", lambda: __import__('megatron')))

# HSTU Attention
checks.append(check("HSTU Attention", lambda: __import__('hstu_attn')))

# DynamicEmb
checks.append(check("DynamicEmb", lambda: __import__('dynamicemb')))

# HSTU CUDA Ops
checks.append(check("HSTU CUDA Ops", lambda: __import__('hstu_cuda_ops')))
checks.append(check("Paged KVCache Ops", lambda: __import__('paged_kvcache_ops')))

# ç»Ÿè®¡
total = len(checks)
passed = sum(checks)

print(f"\né€šè¿‡: {passed}/{total}")

if passed == total:
    print("ğŸ‰ æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼ç¯å¢ƒé…ç½®å®Œæˆï¼")
    sys.exit(0)
else:
    print("âŒ éƒ¨åˆ†æ£€æŸ¥å¤±è´¥ï¼Œè¯·æŸ¥çœ‹ä¸Šè¿°é”™è¯¯ä¿¡æ¯")
    sys.exit(1)
EOF
```

ä¿å­˜ä¸º `check_env.sh` å¹¶è¿è¡Œ:
```bash
bash check_env.sh
```

---

**ç¥æ‚¨é…ç½®é¡ºåˆ©ï¼** ğŸš€

